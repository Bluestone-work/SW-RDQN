{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import copy\n",
    "import pygame\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, grid_map, start, goal, max_steps=50000, goal_list=None):\n",
    "        \"\"\"\n",
    "        初始化 GridWorld 环境\n",
    "        \"\"\"\n",
    "        self.grid_map = grid_map\n",
    "        self.base_grid_map = grid_map.copy()  # 保存基础地图（无动态障碍物）\n",
    "        self.rows, self.cols = grid_map.shape\n",
    "        self.max_steps = max_steps\n",
    "        self.goal_list = goal_list if goal_list else []  # 目标点列表\n",
    "        \n",
    "        if start is not None and goal is not None:\n",
    "            self.start = start\n",
    "            self.goal = goal\n",
    "            self.agent_pos = self.start\n",
    "            self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "        else:\n",
    "            self.reset_dynamic(start, goal)\n",
    "\n",
    "    def reset_dynamic(self, num_obstacles=30, num_dynamic_obstacles=10):\n",
    "        \"\"\"\n",
    "        每次 reset 时动态生成障碍物，并使用传入的起点和目标，或者随机生成。\n",
    "        \"\"\"\n",
    "        self.grid_map = self.base_grid_map.copy()\n",
    "\n",
    "        # 固定障碍物初始化\n",
    "        valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        random_obstacles = random.sample(valid_positions, num_obstacles)\n",
    "        for x, y in random_obstacles:\n",
    "            self.grid_map[x, y] = 1\n",
    "\n",
    "        # 动态障碍物初始化\n",
    "        self.dynamic_obstacles = []\n",
    "        for _ in range(num_dynamic_obstacles):\n",
    "            x, y = random.choice(valid_positions)\n",
    "            direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "            speed = random.randint(1, 2)\n",
    "\n",
    "            self.dynamic_obstacles.append({\n",
    "                'position': (x, y),\n",
    "                'direction': direction,\n",
    "                'speed': speed\n",
    "            })\n",
    "\n",
    "        # 确保起点和目标点不在障碍物中\n",
    "        self.valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        self.agent_pos = self.start\n",
    "        self.steps = 0\n",
    "        self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态\n",
    "        \"\"\"\n",
    "        self.reset_dynamic()\n",
    "        self.goal_list_copy = self.goal_list.copy()\n",
    "        self.current_goal_index = 0\n",
    "        self.goal = self.goal_list[self.current_goal_index] if self.goal_list else None\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        获取过去几帧的状态，生成时间序列。\n",
    "        \"\"\"\n",
    "        nearby_grid = np.ones((7, 7), dtype=int)\n",
    "        x_min, x_max = max(0, self.agent_pos[0] - 3), min(self.rows, self.agent_pos[0] + 4)\n",
    "        y_min, y_max = max(0, self.agent_pos[1] - 3), min(self.cols, self.agent_pos[1] + 4)\n",
    "        r_min, r_max = 3 - (self.agent_pos[0] - x_min), 3 + (x_max - self.agent_pos[0])\n",
    "        c_min, c_max = 3 - (self.agent_pos[1] - y_min), 3 + (y_max - self.agent_pos[1])\n",
    "\n",
    "        nearby_grid[r_min:r_max, c_min:c_max] = self.grid_map[x_min:x_max, y_min:y_max]\n",
    "        nearby_flat = nearby_grid.flatten()\n",
    "\n",
    "        dx = self.goal[0] - self.agent_pos[0]\n",
    "        dy = self.goal[1] - self.agent_pos[1]\n",
    "        distance_to_goal = np.sqrt(dx**2 + dy**2)\n",
    "        angle_to_goal = np.arctan2(dy, dx)\n",
    "\n",
    "        # 当前帧的状态\n",
    "        current_state = np.concatenate(([distance_to_goal, angle_to_goal], nearby_flat))\n",
    "        return current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作并更新环境状态\n",
    "        \"\"\"\n",
    "        actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1), (0 , 0)]\n",
    "        delta = actions[action]\n",
    "        next_pos = (self.agent_pos[0] + delta[0], self.agent_pos[1] + delta[1])\n",
    "\n",
    "        # 检查是否越界或碰到障碍物（包括动态障碍物和固定障碍物）\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward = -3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        if not (0 <= next_pos[0] < self.rows and 0 <= next_pos[1] < self.cols) or self.grid_map[next_pos] == 1:\n",
    "            reward = -3.0\n",
    "            done = True\n",
    "            return self.get_state(), reward, done\n",
    "\n",
    "        self.agent_pos = next_pos\n",
    "        reward = -0.5\n",
    "        done = False\n",
    "\n",
    "        next_distance = np.sqrt((self.goal[0] - self.agent_pos[0])**2 + (self.goal[1] - self.agent_pos[1])**2)\n",
    "        if(self.distance > next_distance):\n",
    "            reward += 0.6\n",
    "        else:\n",
    "            reward -= 0.4\n",
    "        self.distance = next_distance\n",
    "\n",
    "        action_vector = np.array([delta[0], delta[1]])\n",
    "        goal_vector = np.array([self.goal[0] - self.agent_pos[0], self.goal[1] - self.agent_pos[1]])\n",
    "        goal_vector_norm = goal_vector / (np.linalg.norm(goal_vector) + 1e-5)\n",
    "        \n",
    "        if np.linalg.norm(goal_vector) > 0.1:\n",
    "            alignment_reward = np.dot(action_vector, goal_vector_norm)\n",
    "        else:\n",
    "            alignment_reward = 0\n",
    "        \n",
    "        reward += alignment_reward * 0.4\n",
    "\n",
    "        if self.distance < 0.5:\n",
    "            reward += 10\n",
    "            if self.current_goal_index + 1 < len(self.goal_list):\n",
    "                self.current_goal_index += 1\n",
    "                self.goal = self.goal_list[self.current_goal_index]  \n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        # 更新动态障碍物的位置\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            x, y = obstacle['position']\n",
    "            direction = obstacle['direction']\n",
    "            speed = obstacle['speed']\n",
    "            \n",
    "            if direction == 'up':\n",
    "                new_pos = (x - speed, y)\n",
    "            elif direction == 'down':\n",
    "                new_pos = (x + speed, y)\n",
    "            elif direction == 'left':\n",
    "                new_pos = (x, y - speed)\n",
    "            elif direction == 'right':\n",
    "                new_pos = (x, y + speed)\n",
    "            \n",
    "            if 0 <= new_pos[0] < self.rows and 0 <= new_pos[1] < self.cols:\n",
    "                if self.grid_map[new_pos[0], new_pos[1]] == 1:\n",
    "                    if direction == 'up':\n",
    "                        obstacle['direction'] = 'down'\n",
    "                    elif direction == 'down':\n",
    "                        obstacle['direction'] = 'up'\n",
    "                    elif direction == 'left':\n",
    "                        obstacle['direction'] = 'right'\n",
    "                    elif direction == 'right':\n",
    "                        obstacle['direction'] = 'left'\n",
    "                else:\n",
    "                    # 更新障碍物的位置\n",
    "                    self.grid_map[x, y] = 0  # 清除旧位置\n",
    "                    self.grid_map[new_pos[0], new_pos[1]] = 1  # 设置新位置\n",
    "                    obstacle['position'] = new_pos\n",
    "\n",
    "        # 计算最小障碍物距离\n",
    "        min_distance_to_obstacle = float('inf')\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if self.grid_map[i, j] == 1:  # 障碍物\n",
    "                    distance_to_obstacle = np.linalg.norm(np.array(self.agent_pos) - np.array((i, j)))\n",
    "                    min_distance_to_obstacle = min(min_distance_to_obstacle, distance_to_obstacle)\n",
    "\n",
    "        # 奖励：距离障碍物越近，惩罚越大\n",
    "        if min_distance_to_obstacle == 1.0:\n",
    "            reward -= 2.0\n",
    "        elif min_distance_to_obstacle <= 2.0:\n",
    "            reward -= 1.5\n",
    "        elif min_distance_to_obstacle <= 3.0:\n",
    "            reward -= 0.2\n",
    "        else :\n",
    "            reward -= 0.01\n",
    "\n",
    "        # 检查是否碰到动态障碍物\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward -= 3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "            reward -= 10\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, action_dim)  # 输出每个动作的 logits\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        logits = self.l3(a)  # 未经过 softmax\n",
    "        return logits\n",
    "\n",
    "    def get_action_prob(self, state):\n",
    "        logits = self.forward(state)\n",
    "        action_probs = F.softmax(logits, dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "    def sample(self, state):\n",
    "        logits = self.forward(state)\n",
    "        action_probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_probs = dist.log_prob(action)\n",
    "        return action.unsqueeze(1), log_probs.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.action_dim = action_dim  # 存储动作维度\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # 将动作转换为 one-hot 编码\n",
    "        action_one_hot = F.one_hot(action.squeeze(-1), num_classes=self.action_dim).float()\n",
    "        x = torch.cat([state, action_one_hot], dim=-1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        q_value = self.l3(x)\n",
    "        return q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=20000):\n",
    "        self.max_size = max_size\n",
    "        self.storage = []\n",
    "        self.ptr = 0\n",
    "\n",
    "    def append(self, transition):\n",
    "        state, action, reward, next_state, done = transition\n",
    "        if np.any(np.isnan(state)) or np.any(np.isnan(action)) or np.isnan(reward) or np.any(np.isnan(next_state)):\n",
    "            print(\"Invalid data detected in transition. Skipping storage:\")\n",
    "            print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}, Done: {done}\")\n",
    "            return\n",
    "\n",
    "        if len(self.storage) < self.max_size:\n",
    "            self.storage.append(None)\n",
    "        \n",
    "        self.storage[self.ptr] = (state, action, reward, next_state, done)\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for i in indices:\n",
    "            state, action, reward, next_state, done = self.storage[i]\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(action)  # 保持动作为整数\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, state_dim, action_dim, actor_lr=3e-4, critic_lr=3e-4, gamma=0.99, tau=0.005, device=\"cuda\", alpha=0.2, hidden_dim=256, epsilon=0.1, epsilon_min=0.05, epsilon_decay=1e-5):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.critic_1 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic_2 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic_target_1 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic_target_2 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic_target_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_target_2.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer_1 = torch.optim.Adam(self.critic_1.parameters(), lr=critic_lr)\n",
    "        self.critic_optimizer_2 = torch.optim.Adam(self.critic_2.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.alpha = alpha  # 熵权重\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(max_size=1000000)\n",
    "\n",
    "        self.total_it = 0\n",
    "        self.epsilon = epsilon  # 探索率\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actor.l3.out_features - 1)\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        action_probs = self.actor.get_action_prob(state)\n",
    "        action = torch.argmax(action_probs, dim=-1).cpu().data.numpy().flatten()\n",
    "        return int(action[0])  # 返回整数\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def soft_update(self, source, target):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(self.tau * source_param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def train(self, batch_size=256): \n",
    "        if len(self.replay_buffer.storage) < batch_size:\n",
    "            return None, None, None  # 如果样本不足，返回 None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        # 转换为 Tensor\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "\n",
    "        # Critic 更新\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor_target.sample(next_states)\n",
    "            target_Q1 = self.critic_target_1(next_states, next_actions)\n",
    "            target_Q2 = self.critic_target_2(next_states, next_actions)\n",
    "            target_Q = torch.min(target_Q1, target_Q2) - self.alpha * next_log_probs\n",
    "            target_Q = rewards + (1 - dones) * self.gamma * target_Q\n",
    "\n",
    "        current_Q1 = self.critic_1(states, actions)\n",
    "        current_Q2 = self.critic_2(states, actions)\n",
    "\n",
    "        critic_loss_1 = F.mse_loss(current_Q1, target_Q)\n",
    "        critic_loss_2 = F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # 更新 Critic 网络\n",
    "        self.critic_optimizer_1.zero_grad()\n",
    "        critic_loss_1.backward()\n",
    "        self.critic_optimizer_1.step()\n",
    "\n",
    "        self.critic_optimizer_2.zero_grad()\n",
    "        critic_loss_2.backward()\n",
    "        self.critic_optimizer_2.step()\n",
    "\n",
    "        # Actor 更新\n",
    "        new_actions, log_probs = self.actor.sample(states)\n",
    "        Q1_new = self.critic_1(states, new_actions)\n",
    "        Q2_new = self.critic_2(states, new_actions)\n",
    "        Q_min_new = torch.min(Q1_new, Q2_new)\n",
    "        actor_loss = (self.alpha * log_probs - Q_min_new).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新目标网络\n",
    "        self.soft_update(self.actor, self.actor_target)\n",
    "        self.soft_update(self.critic_1, self.critic_target_1)\n",
    "        self.soft_update(self.critic_2, self.critic_target_2)\n",
    "        \n",
    "        # 逐步减少探索率\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        \n",
    "        return actor_loss.item(), critic_loss_1.item(), critic_loss_2.item()\n",
    "\n",
    "    def save_model(self, actor_path=\"actor.pth\", critic_path=\"critic.pth\"):\n",
    "        torch.save(self.actor.state_dict(), actor_path)\n",
    "        torch.save(self.critic_1.state_dict(), critic_path)\n",
    "        torch.save(self.critic_2.state_dict(), critic_path)\n",
    "        print(f\"Actor model saved to {actor_path}\")\n",
    "        print(f\"Critic model saved to {critic_path}\")\n",
    "\n",
    "    def load_model(self, actor_path=\"actor.pth\", critic_path=\"critic.pth\"):\n",
    "        self.actor.load_state_dict(torch.load(actor_path))\n",
    "        self.critic_1.load_state_dict(torch.load(critic_path))\n",
    "        self.critic_2.load_state_dict(torch.load(critic_path))\n",
    "        print(f\"Actor model loaded from {actor_path}\")\n",
    "        print(f\"Critic models loaded from {critic_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=1000, max_steps=1000, actor_path=\"actor.pth\", critic_path=\"critic.pth\", log_dir=\"runs\"):\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = os.path.join(log_dir, f\"run_{run_id}\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            actor_loss, critic_loss_1, critic_loss_2 = agent.train()\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            episode_steps += 1\n",
    "\n",
    "            # Log\n",
    "            writer.add_scalar('train/total_reward', total_reward, episode)\n",
    "            if actor_loss is not None and critic_loss_1 is not None and critic_loss_2 is not None:\n",
    "                writer.add_scalar('train/actor_loss', actor_loss, episode)\n",
    "                writer.add_scalar('train/critic_loss_1', critic_loss_1, episode)\n",
    "                writer.add_scalar('train/critic_loss_2', critic_loss_2, episode)\n",
    "\n",
    "            if episode_steps > max_steps:\n",
    "                break\n",
    "\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            agent.save_model(actor_path=actor_path, critic_path=critic_path)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor model saved to ddpg_actor.pth\n",
      "Critic model saved to ddpg_critic.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m SAC(state_dim, action_dim)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mddpg_actor.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 保存 Actor 的路径\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcritic_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mddpg_critic.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 保存 Critic 的路径\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(env, agent, episodes, max_steps, actor_path, critic_path, log_dir)\u001b[0m\n\u001b[0;32m     17\u001b[0m episode_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 20\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     22\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mSAC.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39ml3\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mget_action_prob(state)\n\u001b[0;32m     35\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(action_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_map = np.loadtxt(\"grid_map_final.txt\", dtype=int)\n",
    "nodes=[(51, 12), (55, 23), (55, 30), (32, 47), (9, 50), (9, 64)]\n",
    "env = GridWorld(grid_map , start=(58, 5) , goal=(51, 12),goal_list=nodes)\n",
    "\n",
    "# 实例化 DDPG Agent\n",
    "state_dim = env.get_state().shape[0]\n",
    "action_dim = 9 \n",
    "agent = SAC(state_dim, action_dim)\n",
    "# 开始训练\n",
    "train_agent(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    episodes=10000,\n",
    "    max_steps=50000,\n",
    "    actor_path=\"ddpg_actor.pth\",  # 保存 Actor 的路径\n",
    "    critic_path=\"ddpg_critic.pth\",  # 保存 Critic 的路径\n",
    "    log_dir=\"runs\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
