{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_map, start, goal, max_steps=50000, goal_list=None):\n",
    "        \"\"\"\n",
    "        初始化 GridWorld 环境\n",
    "        \"\"\"\n",
    "        self.grid_map = grid_map\n",
    "        self.base_grid_map = grid_map.copy()  # 保存基础地图（无动态障碍物）\n",
    "        self.rows, self.cols = grid_map.shape\n",
    "        self.max_steps = max_steps\n",
    "        self.goal_list = goal_list if goal_list else []  # 目标点列表\n",
    "        \n",
    "        if start is not None and goal is not None:\n",
    "            self.start = start\n",
    "            self.goal = goal\n",
    "            self.agent_pos = self.start\n",
    "            self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "        else:\n",
    "            self.reset_dynamic(start, goal)\n",
    "\n",
    "        # 定义可能的动作\n",
    "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1), (0 , 0)]\n",
    "\n",
    "    def reset_dynamic(self, num_obstacles=30, num_dynamic_obstacles=10):\n",
    "        \"\"\"\n",
    "        每次 reset 时动态生成障碍物，并使用传入的起点和目标，或者随机生成。\n",
    "        \"\"\"\n",
    "        self.grid_map = self.base_grid_map.copy()\n",
    "\n",
    "        # 固定障碍物初始化\n",
    "        valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        random_obstacles = random.sample(valid_positions, num_obstacles)\n",
    "        for x, y in random_obstacles:\n",
    "            self.grid_map[x, y] = 1\n",
    "\n",
    "        # 动态障碍物初始化\n",
    "        self.dynamic_obstacles = []\n",
    "        for _ in range(num_dynamic_obstacles):\n",
    "            x, y = random.choice(valid_positions)\n",
    "            direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "            speed = random.randint(1, 2)\n",
    "\n",
    "            self.dynamic_obstacles.append({\n",
    "                'position': (x, y),\n",
    "                'direction': direction,\n",
    "                'speed': speed\n",
    "            })\n",
    "\n",
    "        # 确保起点和目标点不在障碍物中\n",
    "        self.valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        self.agent_pos = self.start\n",
    "        self.steps = 0\n",
    "        self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态\n",
    "        \"\"\"\n",
    "        self.reset_dynamic()\n",
    "        self.goal_list_copy = self.goal_list.copy()\n",
    "        self.current_goal_index = 0\n",
    "        self.goal = self.goal_list[self.current_goal_index] if self.goal_list else None\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        获取当前帧的状态，不使用历史帧。\n",
    "        \"\"\"\n",
    "        nearby_grid = np.ones((7, 7), dtype=int)\n",
    "        x_min, x_max = max(0, self.agent_pos[0] - 3), min(self.rows, self.agent_pos[0] + 4)\n",
    "        y_min, y_max = max(0, self.agent_pos[1] - 3), min(self.cols, self.agent_pos[1] + 4)\n",
    "        r_min, r_max = 3 - (self.agent_pos[0] - x_min), 3 + (x_max - self.agent_pos[0])\n",
    "        c_min, c_max = 3 - (self.agent_pos[1] - y_min), 3 + (y_max - self.agent_pos[1])\n",
    "\n",
    "        nearby_grid[r_min:r_max, c_min:c_max] = self.grid_map[x_min:x_max, y_min:y_max]\n",
    "        nearby_flat = nearby_grid.flatten()\n",
    "\n",
    "        dx = self.goal[0] - self.agent_pos[0]\n",
    "        dy = self.goal[1] - self.agent_pos[1]\n",
    "        distance_to_goal = np.sqrt(dx**2 + dy**2)\n",
    "        angle_to_goal = np.arctan2(dy, dx)\n",
    "\n",
    "        # 当前帧的状态\n",
    "        current_state = np.concatenate(([distance_to_goal, angle_to_goal], nearby_flat))\n",
    "        return current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作并更新环境状态\n",
    "        \"\"\"\n",
    "        state = self.get_state()\n",
    "        distance_to_goal = state[0]  \n",
    "        angle_to_goal = state[1]  \n",
    "        nearby_flat = state[2:]\n",
    "\n",
    "        actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1), (0 , 0)]\n",
    "        delta = actions[action]\n",
    "        next_pos = (self.agent_pos[0] + delta[0], self.agent_pos[1] + delta[1])\n",
    "\n",
    "        # 检查是否越界或碰到障碍物（包括动态障碍物和固定障碍物）\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward = -3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        if not (0 <= next_pos[0] < self.rows and 0 <= next_pos[1] < self.cols) or self.grid_map[next_pos] == 1:\n",
    "            reward = -3.0\n",
    "            done = True\n",
    "            return self.get_state(), reward, done\n",
    "\n",
    "        self.agent_pos = next_pos\n",
    "        reward = -0.5\n",
    "        done = False\n",
    "\n",
    "        next_distance = np.sqrt((self.goal[0] - self.agent_pos[0])**2 + (self.goal[1] - self.agent_pos[1])**2)\n",
    "        if(self.distance > next_distance):\n",
    "            reward += 0.6\n",
    "        else:\n",
    "            reward -= 0.4\n",
    "        self.distance = next_distance\n",
    "\n",
    "        action_vector = np.array([delta[0], delta[1]])\n",
    "        goal_vector = np.array([self.goal[0] - self.agent_pos[0], self.goal[1] - self.agent_pos[1]])\n",
    "        goal_vector_norm = goal_vector / (np.linalg.norm(goal_vector) + 1e-5)\n",
    "        \n",
    "        if np.linalg.norm(goal_vector) > 0.1:\n",
    "            alignment_reward = np.dot(action_vector, goal_vector_norm)\n",
    "        else:\n",
    "            alignment_reward = 0\n",
    "        \n",
    "        reward += alignment_reward * 0.4\n",
    "\n",
    "        if self.distance < 0.5:\n",
    "            reward += 10\n",
    "            if self.current_goal_index + 1 < len(self.goal_list):\n",
    "                self.current_goal_index += 1\n",
    "                self.goal = self.goal_list[self.current_goal_index]  \n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        # 更新动态障碍物的位置\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            x, y = obstacle['position']\n",
    "            direction = obstacle['direction']\n",
    "            speed = obstacle['speed']\n",
    "            \n",
    "            if direction == 'up':\n",
    "                new_pos = (x - speed, y)\n",
    "            elif direction == 'down':\n",
    "                new_pos = (x + speed, y)\n",
    "            elif direction == 'left':\n",
    "                new_pos = (x, y - speed)\n",
    "            elif direction == 'right':\n",
    "                new_pos = (x, y + speed)\n",
    "            \n",
    "            if 0 <= new_pos[0] < self.rows and 0 <= new_pos[1] < self.cols:\n",
    "                if self.grid_map[new_pos[0], new_pos[1]] == 1:\n",
    "                    if direction == 'up':\n",
    "                        obstacle['direction'] = 'down'\n",
    "                    elif direction == 'down':\n",
    "                        obstacle['direction'] = 'up'\n",
    "                    elif direction == 'left':\n",
    "                        obstacle['direction'] = 'right'\n",
    "                    elif direction == 'right':\n",
    "                        obstacle['direction'] = 'left'\n",
    "                else:\n",
    "                    # 更新障碍物的位置\n",
    "                    self.grid_map[x, y] = 0  # 清除旧位置\n",
    "                    self.grid_map[new_pos[0], new_pos[1]] = 1  # 设置新位置\n",
    "                    obstacle['position'] = new_pos\n",
    "\n",
    "        # 计算最小障碍物距离\n",
    "        min_distance_to_obstacle = float('inf')\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if self.grid_map[i, j] == 1:  # 障碍物\n",
    "                    distance_to_obstacle = np.linalg.norm(np.array(self.agent_pos) - np.array((i, j)))\n",
    "                    min_distance_to_obstacle = min(min_distance_to_obstacle, distance_to_obstacle)\n",
    "\n",
    "        # 奖励：距离障碍物越近，惩罚越大\n",
    "        if min_distance_to_obstacle == 1.0:\n",
    "            reward -= 2.0\n",
    "        elif min_distance_to_obstacle <= 2.0:\n",
    "            reward -= 1.5\n",
    "        elif min_distance_to_obstacle <= 3.0:\n",
    "            reward -= 0.2\n",
    "        else :\n",
    "            reward -= 0.01\n",
    "\n",
    "        # 检查是否碰到动态障碍物\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward -= 3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "            reward -= 10\n",
    "\n",
    "        return self.get_state(), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class reGridWorld:\n",
    "    def __init__(self, grid_map, start, goal, max_steps=50000, goal_list=None):\n",
    "        \"\"\"\n",
    "        初始化 GridWorld 环境\n",
    "        \"\"\"\n",
    "        self.grid_map = grid_map\n",
    "        self.base_grid_map = grid_map.copy()  # 保存基础地图（无动态障碍物）\n",
    "        self.rows, self.cols = grid_map.shape\n",
    "        self.max_steps = max_steps\n",
    "        self.goal_list = goal_list if goal_list else []  # 目标点列表\n",
    "        self.history = deque(maxlen=4)  # 保存过去3帧状态\n",
    "        \n",
    "        if start is not None and goal is not None:\n",
    "            self.start = start\n",
    "            self.goal = goal\n",
    "            self.agent_pos = self.start\n",
    "            self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "        else:\n",
    "            self.reset_dynamic(start, goal)\n",
    "\n",
    "    def reset_dynamic(self, num_obstacles=30, num_dynamic_obstacles=10):\n",
    "        \"\"\"\n",
    "        每次 reset 时动态生成障碍物，并使用传入的起点和目标，或者随机生成。\n",
    "        \"\"\"\n",
    "        self.grid_map = self.base_grid_map.copy()\n",
    "\n",
    "        # 固定障碍物初始化\n",
    "        valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        random_obstacles = random.sample(valid_positions, num_obstacles)\n",
    "        for x, y in random_obstacles:\n",
    "            self.grid_map[x, y] = 1\n",
    "\n",
    "        # 动态障碍物初始化\n",
    "        self.dynamic_obstacles = []\n",
    "        for _ in range(num_dynamic_obstacles):\n",
    "            x, y = random.choice(valid_positions)\n",
    "            direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "            speed = random.randint(1, 2)\n",
    "\n",
    "            self.dynamic_obstacles.append({\n",
    "                'position': (x, y),\n",
    "                'direction': direction,\n",
    "                'speed': speed\n",
    "            })\n",
    "\n",
    "        # 确保起点和目标点不在障碍物中\n",
    "        self.valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        self.agent_pos = self.start\n",
    "        self.steps = 0\n",
    "        self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态\n",
    "        \"\"\"\n",
    "        self.reset_dynamic()\n",
    "        self.goal_list_copy = self.goal_list.copy()\n",
    "        self.current_goal_index = 0\n",
    "        self.goal = self.goal_list[self.current_goal_index] if self.goal_list else None\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self, n_frames=4):\n",
    "        \"\"\"\n",
    "        获取过去几帧的状态，生成时间序列。\n",
    "        \"\"\"\n",
    "        nearby_grid = np.ones((7, 7), dtype=int)\n",
    "        x_min, x_max = max(0, self.agent_pos[0] - 3), min(self.rows, self.agent_pos[0] + 4)\n",
    "        y_min, y_max = max(0, self.agent_pos[1] - 3), min(self.cols, self.agent_pos[1] + 4)\n",
    "        r_min, r_max = 3 - (self.agent_pos[0] - x_min), 3 + (x_max - self.agent_pos[0])\n",
    "        c_min, c_max = 3 - (self.agent_pos[1] - y_min), 3 + (y_max - self.agent_pos[1])\n",
    "\n",
    "        nearby_grid[r_min:r_max, c_min:c_max] = self.grid_map[x_min:x_max, y_min:y_max]\n",
    "        nearby_flat = nearby_grid.flatten()\n",
    "\n",
    "        dx = self.goal[0] - self.agent_pos[0]\n",
    "        dy = self.goal[1] - self.agent_pos[1]\n",
    "        distance_to_goal = np.sqrt(dx**2 + dy**2)\n",
    "        angle_to_goal = np.arctan2(dy, dx)\n",
    "\n",
    "        # 当前帧的状态\n",
    "        current_state = np.concatenate(([distance_to_goal, angle_to_goal], nearby_flat))\n",
    "\n",
    "        # 更新历史状态队列\n",
    "        self.history.append(current_state)\n",
    "\n",
    "        # 如果历史帧数不足 n_frames，则用当前帧填充\n",
    "        while len(self.history) < n_frames:\n",
    "            self.history.appendleft(current_state)\n",
    "\n",
    "        # 返回过去 n 帧的状态序列\n",
    "        return np.array(self.history)  # 这里返回的是一个形状为 (n_frames, state_dim) 的二维数组\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作并更新环境状态\n",
    "        \"\"\"\n",
    "        state = self.get_state()\n",
    "        distance_to_goal = state[0]  \n",
    "        angle_to_goal = state[1]  \n",
    "        nearby_flat = state[2:]\n",
    "\n",
    "        actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1), (0 , 0)]\n",
    "        delta = actions[action]\n",
    "        next_pos = (self.agent_pos[0] + delta[0], self.agent_pos[1] + delta[1])\n",
    "\n",
    "        # 检查是否越界或碰到障碍物（包括动态障碍物和固定障碍物）\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward = -3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        if not (0 <= next_pos[0] < self.rows and 0 <= next_pos[1] < self.cols) or self.grid_map[next_pos] == 1:\n",
    "            reward = -3.0\n",
    "            done = True\n",
    "            return self.get_state(), reward, done\n",
    "\n",
    "        self.agent_pos = next_pos\n",
    "        reward = -0.5\n",
    "        done = False\n",
    "\n",
    "        next_distance = np.sqrt((self.goal[0] - self.agent_pos[0])**2 + (self.goal[1] - self.agent_pos[1])**2)\n",
    "        if(self.distance > next_distance):\n",
    "            reward += 0.6\n",
    "        else:\n",
    "            reward -= 0.4\n",
    "        self.distance = next_distance\n",
    "\n",
    "        action_vector = np.array([delta[0], delta[1]])\n",
    "        goal_vector = np.array([self.goal[0] - self.agent_pos[0], self.goal[1] - self.agent_pos[1]])\n",
    "        goal_vector_norm = goal_vector / (np.linalg.norm(goal_vector) + 1e-5)\n",
    "        \n",
    "        if np.linalg.norm(goal_vector) > 0.1:\n",
    "            alignment_reward = np.dot(action_vector, goal_vector_norm)\n",
    "        else:\n",
    "            alignment_reward = 0\n",
    "        \n",
    "        reward += alignment_reward * 0.4\n",
    "\n",
    "        if self.distance < 0.5:\n",
    "            reward += 10\n",
    "            if self.current_goal_index + 1 < len(self.goal_list):\n",
    "                self.current_goal_index += 1\n",
    "                self.goal = self.goal_list[self.current_goal_index]  \n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        # 更新动态障碍物的位置\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            x, y = obstacle['position']\n",
    "            direction = obstacle['direction']\n",
    "            speed = obstacle['speed']\n",
    "            \n",
    "            if direction == 'up':\n",
    "                new_pos = (x - speed, y)\n",
    "            elif direction == 'down':\n",
    "                new_pos = (x + speed, y)\n",
    "            elif direction == 'left':\n",
    "                new_pos = (x, y - speed)\n",
    "            elif direction == 'right':\n",
    "                new_pos = (x, y + speed)\n",
    "            \n",
    "            if 0 <= new_pos[0] < self.rows and 0 <= new_pos[1] < self.cols:\n",
    "                if self.grid_map[new_pos[0], new_pos[1]] == 1:\n",
    "                    if direction == 'up':\n",
    "                        obstacle['direction'] = 'down'\n",
    "                    elif direction == 'down':\n",
    "                        obstacle['direction'] = 'up'\n",
    "                    elif direction == 'left':\n",
    "                        obstacle['direction'] = 'right'\n",
    "                    elif direction == 'right':\n",
    "                        obstacle['direction'] = 'left'\n",
    "                else:\n",
    "                    # 更新障碍物的位置\n",
    "                    self.grid_map[x, y] = 0  # 清除旧位置\n",
    "                    self.grid_map[new_pos[0], new_pos[1]] = 1  # 设置新位置\n",
    "                    obstacle['position'] = new_pos\n",
    "\n",
    "        # 计算最小障碍物距离\n",
    "        min_distance_to_obstacle = float('inf')\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if self.grid_map[i, j] == 1:  # 障碍物\n",
    "                    distance_to_obstacle = np.linalg.norm(np.array(self.agent_pos) - np.array((i, j)))\n",
    "                    min_distance_to_obstacle = min(min_distance_to_obstacle, distance_to_obstacle)\n",
    "\n",
    "        # 奖励：距离障碍物越近，惩罚越大\n",
    "        if min_distance_to_obstacle == 1.0:\n",
    "            reward -= 2.0\n",
    "        elif min_distance_to_obstacle <= 2.0:\n",
    "            reward -= 1.5\n",
    "        elif min_distance_to_obstacle <= 3.0:\n",
    "            reward -= 0.2\n",
    "        else :\n",
    "            reward -= 0.01\n",
    "\n",
    "        # 检查是否碰到动态障碍物\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward -= 3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "            reward -= 10\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN_with_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, lstm_layers=1):\n",
    "        super(DuelingDQN_with_LSTM, self).__init__()\n",
    "        \n",
    "        # 输入层到隐藏层的全连接\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # 输入到128维的全连接层\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_dim, num_layers=lstm_layers, batch_first=True)\n",
    "        \n",
    "        # 状态价值（V(s)）分支\n",
    "        self.value_fc = nn.Linear(hidden_dim, 128)  # 将 hidden_dim 改为 128\n",
    "        self.value_output = nn.Linear(128, 1)  # 状态的价值\n",
    "        \n",
    "        # 优势（A(s, a)）分支\n",
    "        self.advantage_fc = nn.Linear(hidden_dim, 128)  # 将 hidden_dim 改为 128\n",
    "        self.advantage_output = nn.Linear(128, output_dim)  # 输出维度\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, _ = x.size()  # 获取输入的维度\n",
    "        \n",
    "        # 前向传播\n",
    "        x = torch.relu(self.fc1(x))  # 经过第一层全连接层\n",
    "        \n",
    "        # LSTM层处理\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)  # LSTM处理\n",
    "        \n",
    "        # 使用最后一个时间步的输出\n",
    "        lstm_out_last = lstm_out[:, -1, :]  # 只取最后一个时间步的输出\n",
    "        \n",
    "        # 计算状态价值 V(s)\n",
    "        value = torch.relu(self.value_fc(lstm_out_last))\n",
    "        value = self.value_output(value)\n",
    "        \n",
    "        # 计算动作优势 A(s, a)\n",
    "        advantage = torch.relu(self.advantage_fc(lstm_out_last))\n",
    "        advantage = self.advantage_output(advantage)\n",
    "        \n",
    "        # 组合 V(s) 和 A(s, a) 得到 Q(s, a)\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))  # 这里减去均值是为了稳定性\n",
    "        \n",
    "        return q_values  # 返回Q值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN_with_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, lstm_layers=1):\n",
    "        super(DQN_with_LSTM, self).__init__()\n",
    "        \n",
    "        # 输入层到隐藏层的全连接\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # 输入到128维的全连接层\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_dim, num_layers=lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Q值的输出层\n",
    "        self.q_fc = nn.Linear(hidden_dim, output_dim)  # 最终的 Q 值输出，维度为动作空间大小\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, _ = x.size()\n",
    "        \n",
    "        # 前向传播\n",
    "        x = torch.relu(self.fc1(x))  # 经过第一层全连接层\n",
    "        \n",
    "        # LSTM层处理\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)  # LSTM处理\n",
    "        \n",
    "        # 使用最后一个时间步的输出\n",
    "        lstm_out_last = lstm_out[:, -1, :]  # 只取最后一个时间步的输出\n",
    "        \n",
    "        # 计算 Q 值\n",
    "        q_values = self.q_fc(lstm_out_last)\n",
    "        \n",
    "        return q_values  # 返回Q值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # 隐藏层到输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # 通过ReLU激活函数\n",
    "        q_values = self.fc2(x)  # 输出Q值\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN:\n",
    "    def __init__(self, env, gamma=0.8, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9, \n",
    "                learning_rate=4e-4, batch_size=64, memory_size=10000, device=None, alpha=0.6, beta=0.4,\n",
    "                n_step=4, shared_replay_buffer=None):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha  # 优先级的重要性\n",
    "        self.beta = beta    # 用于优先级采样的偏差修正\n",
    "        self.n_step = n_step  # Multi-step 参数\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 使用 Dueling DQN 网络\n",
    "        self.q_network = DQN(51, 9).to(self.device)  # 输入51是特征维度，输出9是动作维度\n",
    "        self.target_network = DQN(51, 9).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # 使用共享缓冲区（如果提供），否则实例化私有缓冲区\n",
    "        self.memory = shared_replay_buffer or deque(maxlen=memory_size)\n",
    "        self.priority_sum = 0  # 初始化总优先级为 0\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 8)\n",
    "        \n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        state_tensor = state_tensor.unsqueeze(0)  # (1, n_frames, state_dim) 转换为三维张量\n",
    "        \n",
    "        q_values = self.q_network(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        # 计算多步TD误差\n",
    "        td_error = 0.0  # 初始时，TD error 为 0\n",
    "\n",
    "        if len(self.memory) > 0:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0)  # (1, n_frames, state_dim)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device).unsqueeze(0)  # (1, n_frames, state_dim)\n",
    "            \n",
    "            # 计算 Q 值和 TD Error\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long).to(self.device)\n",
    "            current_q_values = q_values.gather(1, action_tensor.view(-1, 1)).squeeze()\n",
    "            next_q_values = self.target_network(next_state_tensor).max()\n",
    "            td_error = abs(reward + self.gamma * next_q_values.item() - current_q_values.item())\n",
    "        \n",
    "        # 存储经验并附加优先级\n",
    "        priority = (td_error + 1e-5) ** self.alpha  # 1e-5 防止优先级为零\n",
    "        self.memory.append((state, action, reward, next_state, done, priority))\n",
    "        self.priority_sum += priority\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # 使用均匀采样\n",
    "        indices = random.sample(range(len(self.memory)), self.batch_size)\n",
    "        batch = [self.memory[idx] for idx in indices]\n",
    "        # 使用优先级采样\n",
    "        # probabilities = np.array([transition[5] for transition in self.memory])\n",
    "        # probabilities += 1e-5  # 防止某些优先级为零\n",
    "        # probabilities /= probabilities.sum()  # 归一化概率总和为1\n",
    "        # indices = np.random.choice(len(self.memory), self.batch_size, p=probabilities)\n",
    "        # batch = [self.memory[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones, priorities = zip(*batch)\n",
    "        \n",
    "        # 转换为 Tensor\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # 确保状态是三维的\n",
    "        if states.dim() == 2:\n",
    "            states = states.unsqueeze(0)  # (1, n_frames, state_dim)\n",
    "        if next_states.dim() == 2:\n",
    "            next_states = next_states.unsqueeze(0)  # (1, n_frames, state_dim)\n",
    "\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # 计算多步奖励目标\n",
    "        target_q_values = rewards\n",
    "        for i in range(1, self.n_step):\n",
    "            target_q_values += (self.gamma ** i) * rewards\n",
    "\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        target_q_values += (self.gamma ** self.n_step) * next_q_values * (1 - dones)\n",
    "\n",
    "        # 计算损失\n",
    "        current_q_values = self.q_network(states).gather(1, actions.view(-1, 1)).squeeze()\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        # 反向传播\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 更新目标网络\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update(self):\n",
    "        self.update_target_network()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, file_path=\"dqn_model.pth\"):\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "    \n",
    "    def load_model(self, file_path=\"dqn_model.pth\"):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        print(f\"Model loaded from {file_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(env, agent, episodes=1000, max_steps=1000, save_path=\"dqn_model.pth\"):\n",
    "    import pygame\n",
    "    import numpy as np\n",
    "\n",
    "    # 初始化 Pygame\n",
    "    pygame.init()\n",
    "    grid_size = 10  # 每个格子的像素大小\n",
    "    screen = pygame.display.set_mode((env.cols * grid_size, env.rows * grid_size))\n",
    "    pygame.display.set_caption(\"Training Visualization\")\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    # 初始化字体\n",
    "    pygame.font.init()\n",
    "    font = pygame.font.SysFont(\"Arial\", 24)\n",
    "\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        env.reset_dynamic(num_obstacles=min(5 + episode // 100, 20))  # 动态调整障碍物数量\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        path = [env.agent_pos]  # 路径记录\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # 渲染环境\n",
    "            screen.fill((255, 255, 255))  # 清空屏幕，设置白色背景\n",
    "\n",
    "            # 绘制栅格地图\n",
    "            for i in range(env.rows):\n",
    "                for j in range(env.cols):\n",
    "                    color = (255, 255, 255)  # 默认白色\n",
    "                    if env.grid_map[i, j] == 1:\n",
    "                        color = (0, 0, 0)  # 黑色障碍物\n",
    "                    pygame.draw.rect(screen, color, (j * grid_size, i * grid_size, grid_size, grid_size))\n",
    "\n",
    "            # 绘制目标\n",
    "            pygame.draw.rect(\n",
    "                screen,\n",
    "                (0, 0, 255),\n",
    "                (env.goal[1] * grid_size, env.goal[0] * grid_size, grid_size, grid_size)\n",
    "            )  # 蓝色目标点\n",
    "\n",
    "            # 绘制路径轨迹\n",
    "            for pos in path:\n",
    "                pygame.draw.circle(\n",
    "                    screen,\n",
    "                    (200, 200, 200),  # 浅灰色轨迹\n",
    "                    (int(pos[1] * grid_size + grid_size // 2), int(pos[0] * grid_size + grid_size // 2)),\n",
    "                    5\n",
    "                )\n",
    "\n",
    "            # 绘制智能体\n",
    "            car_x = int(env.agent_pos[1] * grid_size + grid_size // 2)\n",
    "            car_y = int(env.agent_pos[0] * grid_size + grid_size // 2)\n",
    "            car_radius = grid_size // 4\n",
    "            pygame.draw.circle(screen, (255, 0, 0), (car_x, car_y), car_radius)\n",
    "\n",
    "            # 智能体选择动作\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(agent.device)\n",
    "            action = agent.select_action(state_tensor)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "\n",
    "            # 更新状态和累计奖励\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path.append(env.agent_pos)  # 记录路径\n",
    "            steps += 1\n",
    "\n",
    "            # 显示单步奖励\n",
    "            reward_text = font.render(f\"Episode: {episode}\", True, (0, 0, 0))\n",
    "            screen.blit(reward_text, (10, 10))  # 在屏幕左上角显示\n",
    "\n",
    "            # 显示总奖励\n",
    "            total_reward_text = font.render(f\"Total Reward: {total_reward:.2f}\", True, (0, 0, 0))\n",
    "            screen.blit(total_reward_text, (10, 40))  # 显示总奖励\n",
    "\n",
    "            pygame.display.flip()  # 更新显示\n",
    "\n",
    "            # 检查是否按下退出事件\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "\n",
    "            # 限制帧率\n",
    "            clock.tick(60)\n",
    "\n",
    "        # 记录最佳模型\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            agent.save_model(save_path)\n",
    "\n",
    "        # 每 10 轮打印日志\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}: Total Reward = {total_reward:.2f}\")\n",
    "\n",
    "    pygame.quit()\n",
    "    print(\"Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train_agent(env, agent, episodes=1000, max_steps=1000, save_path=\"dqn_model.pth\", log_dir=\"runs\"): \n",
    "    # 初始化 TensorBoard 日志记录器\n",
    "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = os.path.join(log_dir, f\"run_{run_id}\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    best_reward = -float('inf')  # 记录最佳奖励\n",
    "    step_counter = 0  # 全局步数计数器\n",
    "    all_rewards = []  # 保存所有回合奖励\n",
    "    all_losses = []  # 保存所有回合平均损失\n",
    "    all_epsilons = []  # 保存所有回合探索率\n",
    "\n",
    "    # 使用 tqdm 进度条显示训练过程\n",
    "    with tqdm(total=episodes, desc=\"训练进度\", unit=\"episode\") as pbar:\n",
    "        for episode in range(episodes):\n",
    "            env.reset_dynamic()  # 重置环境动态障碍物\n",
    "            state = env.reset()  # 获取初始状态\n",
    "            done = False\n",
    "            total_reward = 0  # 当前回合总奖励\n",
    "            episode_steps = 0  # 当前回合步数计数\n",
    "            losses = []  # 当前回合的损失列表\n",
    "            q_values = []  # 当前回合的 Q 值记录\n",
    "\n",
    "            while not done:\n",
    "                # 转换状态为 Tensor\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).to(agent.device)\n",
    "                \n",
    "                # 智能体选择动作\n",
    "                action = agent.select_action(state_tensor)\n",
    "                \n",
    "                # 环境执行动作并返回新状态、奖励和是否结束\n",
    "                next_state, reward, done = env.step(action)\n",
    "                \n",
    "                # 存储经验和训练模型\n",
    "                agent.store_transition(state, action, reward, next_state, done)\n",
    "                loss = agent.train()  # 返回当前训练的损失\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                episode_steps += 1\n",
    "                step_counter += 1\n",
    "\n",
    "                # 记录损失和 Q 值分布\n",
    "                if loss is not None:\n",
    "                    losses.append(loss.item())\n",
    "                with torch.no_grad():\n",
    "                    q_values.append(agent.q_network(state_tensor.unsqueeze(0)).cpu().numpy().flatten())\n",
    "\n",
    "                # 如果达到最大步数，则结束当前回合\n",
    "                if episode_steps >= max_steps:\n",
    "                    done = True\n",
    "\n",
    "            # 每个回合结束后，衰减 epsilon（探索率）\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "            # 记录奖励、epsilon 和损失到 TensorBoard\n",
    "            writer.add_scalar(\"Reward/Episode\", total_reward, episode)\n",
    "            writer.add_scalar(\"Epsilon\", agent.epsilon, episode)\n",
    "            if losses:\n",
    "                avg_loss = np.mean(losses)\n",
    "                writer.add_scalar(\"Loss/Episode\", avg_loss, episode)\n",
    "                all_losses.append(avg_loss)\n",
    "            all_rewards.append(total_reward)\n",
    "            all_epsilons.append(agent.epsilon)\n",
    "\n",
    "            # 记录 Q 值分布\n",
    "            if q_values:\n",
    "                q_values_flat = np.concatenate(q_values)\n",
    "                writer.add_histogram(\"Q_values/Distribution\", q_values_flat, episode)\n",
    "\n",
    "            # 更新 tqdm 进度条信息\n",
    "            pbar.set_postfix({\"总奖励\": total_reward, \"Epsilon\": round(agent.epsilon, 3)})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # 如果当前回合的奖励是历史最高，则保存模型\n",
    "            if total_reward > best_reward:\n",
    "                best_reward = total_reward\n",
    "                agent.save_model(save_path)\n",
    "\n",
    "            # 每 10 个回合刷新 TensorBoard 数据\n",
    "            if episode % 10 == 0:\n",
    "                writer.flush()\n",
    "\n",
    "        # 保存最终模型\n",
    "        agent.save_model(save_path)\n",
    "\n",
    "        # 关闭 TensorBoard 日志记录器\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"训练完成，模型已保存到 {save_path}\")\n",
    "\n",
    "    # 保存奖励、损失和 epsilon 数据以便后续绘图\n",
    "    np.save(os.path.join(log_dir, \"rewards.npy\"), all_rewards)\n",
    "    np.save(os.path.join(log_dir, \"losses.npy\"), all_losses)\n",
    "    np.save(os.path.join(log_dir, \"epsilons.npy\"), all_epsilons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trained_model(env, agent, model_path, max_steps=1000):\n",
    "    \"\"\"\n",
    "    交互式测试训练好的模型，动态添加障碍物来评估避障能力。\n",
    "\n",
    "    :param env: 环境实例 (GridWorld)\n",
    "    :param agent: 智能体实例\n",
    "    :param model_path: 已训练好的模型文件路径\n",
    "    :param max_steps: 每次测试的最大步数\n",
    "    \"\"\"\n",
    "    # 加载训练好的模型\n",
    "    agent.load_model(model_path)\n",
    "    start_time = time.time()\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "    # 设置 epsilon 为 0（测试时只选择最优动作）\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    # 初始化 Pygame\n",
    "    pygame.init()\n",
    "    grid_size = 10  # 每个格子的像素大小\n",
    "    screen = pygame.display.set_mode((env.cols * grid_size, env.rows * grid_size))\n",
    "    pygame.display.set_caption(\"Trained Model Test\")\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    # 初始化字体\n",
    "    pygame.font.init()\n",
    "    font = pygame.font.SysFont(\"Arial\", 24)\n",
    "\n",
    "    # 测试\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    path = [env.agent_pos]  # 路径记录\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        # 渲染环境\n",
    "        screen.fill((255, 255, 255))  # 清空屏幕，设置白色背景\n",
    "\n",
    "        # 绘制栅格地图\n",
    "        for i in range(env.rows):\n",
    "            for j in range(env.cols):\n",
    "                color = (255, 255, 255)  # 默认白色\n",
    "                if env.grid_map[i, j] == 1:\n",
    "                    color = (0, 0, 0)  # 黑色障碍物\n",
    "                pygame.draw.rect(screen, color, (j * grid_size, i * grid_size, grid_size, grid_size))\n",
    "\n",
    "        # 绘制目标\n",
    "        pygame.draw.rect(\n",
    "            screen,\n",
    "            (0, 0, 255),\n",
    "            (env.goal[1] * grid_size, env.goal[0] * grid_size, grid_size, grid_size)\n",
    "        )  # 蓝色目标点\n",
    "\n",
    "        # 绘制路径轨迹\n",
    "        for pos in path:\n",
    "            pygame.draw.rect(\n",
    "                screen,\n",
    "                (200, 200, 200),  # 浅灰色轨迹\n",
    "                (int(pos[1] * grid_size), int(pos[0] * grid_size), grid_size, grid_size)  # 使用矩形表示路径\n",
    "            )\n",
    "\n",
    "        # 绘制智能体\n",
    "        car_x = int(env.agent_pos[1] * grid_size + grid_size // 2)\n",
    "        car_y = int(env.agent_pos[0] * grid_size + grid_size // 2)\n",
    "        car_radius = grid_size // 4\n",
    "\n",
    "        # 绘制小车的主体\n",
    "        pygame.draw.circle(screen, (255, 0, 0), (car_x, car_y), car_radius)\n",
    "\n",
    "        # 智能体选择动作\n",
    "        action = agent.select_action(state)  # 测试时选择最优动作\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        pygame.display.flip()  # 更新显示\n",
    "\n",
    "        # 更新状态和累计奖励\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        path.append(env.agent_pos)  # 记录路径\n",
    "        steps += 1\n",
    "\n",
    "        # 检查是否按下退出事件或者添加障碍物\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                # 获取鼠标点击的坐标\n",
    "                mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                # 转换为对应的栅格坐标\n",
    "                grid_x = mouse_x // grid_size\n",
    "                grid_y = mouse_y // grid_size\n",
    "                # 在地图中添加障碍物\n",
    "                if env.grid_map[grid_y, grid_x] == 0:  # 只在空白位置添加障碍物\n",
    "                    env.grid_map[grid_y, grid_x] = 1\n",
    "                    print(f\"Obstacle added at ({grid_y}, {grid_x})\")\n",
    "        \n",
    "        # 限制帧率\n",
    "        clock.tick(0.7)\n",
    "    end_time = time.time()\n",
    "    pygame.quit()  # 测试完成后关闭窗口\n",
    "\n",
    "    # 输出测试结果\n",
    "    print(f\"Test Completed. Total Reward: {total_reward}, Steps Taken: {steps}\")\n",
    "    print(f\"仿真完成，耗时 {end_time - start_time:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trained_model_num(env, agent, model_path, max_steps=1000, num_runs=100): \n",
    "    \"\"\"\n",
    "    交互式测试训练好的模型，动态添加障碍物来评估避障能力，并统计成功的次数。\n",
    "\n",
    "    :param env: 环境实例 (GridWorld)\n",
    "    :param agent: 智能体实例\n",
    "    :param model_path: 已训练好的模型文件路径\n",
    "    :param max_steps: 每次测试的最大步数\n",
    "    :param num_runs: 测试的总次数\n",
    "    \"\"\"\n",
    "    # 加载训练好的模型\n",
    "    agent.load_model(model_path)\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "    # 设置 epsilon 为 0（测试时只选择最优动作）\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    success_count = 0  # 记录成功次数\n",
    "\n",
    "    # 测试多次\n",
    "    for run in range(num_runs):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        path = [env.agent_pos]  # 路径记录\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # 智能体选择动作\n",
    "            action = agent.select_action(state)  # 测试时选择最优动作\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # 更新状态和累计奖励\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path.append(env.agent_pos)  # 记录路径\n",
    "            steps += 1\n",
    "\n",
    "            # 检查是否到达目标\n",
    "            if env.agent_pos == env.goal:\n",
    "                success_count += 1\n",
    "                break  # 达到目标则结束当前测试\n",
    "\n",
    "    # 输出成功次数\n",
    "    print(f\"Test Completed. Success Count: {success_count}/{num_runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网格地图\n",
    "grid_map = np.zeros((60, 60))  # 60x60 的空地图\n",
    "grid_map[15, 15] = 1  # 设置障碍物\n",
    "grid_map[14, 15] = 1\n",
    "grid_map[15, 14] = 1\n",
    "grid_map[7, 7] = 1\n",
    "grid_map[17, 12] = 1\n",
    "grid_map[18 , 16] = 1\n",
    "grid_map[24 , 14] =1\n",
    "\n",
    "# 边界障碍物\n",
    "grid_map[:, 0] = 1\n",
    "grid_map[:, -1] = 1\n",
    "grid_map[0, :] = 1\n",
    "grid_map[-1, :] = 1\n",
    "\n",
    "# 实例化 GridWorld 环境\n",
    "#env = GridWorld(grid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 2/2000 [00:00<01:29, 22.39episode/s, 总奖励=-3, Epsilon=0.985]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_39256\\1434181555.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
      "训练进度:   0%|          | 5/2000 [00:00<03:40,  9.05episode/s, 总奖励=-5.98, Epsilon=0.975]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (9) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m agent \u001b[38;5;241m=\u001b[39m RainbowDQN(\n\u001b[0;32m      7\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m      8\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     memory_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 训练智能体\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdqn_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#print(self.goal ,self.agent_pos , self.distance)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[61], line 44\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(env, agent, episodes, max_steps, save_path, log_dir)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 存储经验和训练模型\u001b[39;00m\n\u001b[0;32m     43\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[1;32m---> 44\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 返回当前训练的损失\u001b[39;00m\n\u001b[0;32m     45\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     46\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[59], line 94\u001b[0m, in \u001b[0;36mRainbowDQN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m     target_q_values \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i) \u001b[38;5;241m*\u001b[39m rewards\n\u001b[0;32m     93\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_network(next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 94\u001b[0m target_q_values \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_q_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[0;32m     97\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network(states)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "grid_map = np.loadtxt(\"grid_map_final.txt\", dtype=int)\n",
    "#nodes = [(58, 5), (51, 12), (55, 23), (55, 30), (32, 47), (9, 50), (9, 64)]\n",
    "nodes=[(51, 12), (55, 23), (55, 30), (32, 47), (9, 50), (9, 64)]\n",
    "env = GridWorld(grid_map , start=(58, 5) , goal=(51, 12),goal_list=nodes)\n",
    "# 实例化 RainbowDQN\n",
    "agent = RainbowDQN(\n",
    "    env=env,\n",
    "    gamma=0.9,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.0001,\n",
    "    epsilon_decay=0.995,\n",
    "    learning_rate=0.005,\n",
    "    batch_size=64,\n",
    "    memory_size=200000\n",
    ")\n",
    "# 训练智能体\n",
    "train_agent(env, agent, episodes=2000, max_steps=50000, save_path=\"dqn_model.pth\", log_dir=\"runs\")\n",
    "#print(self.goal ,self.agent_pos , self.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_34892\\3216142474.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Total Reward: 9.64344713487131, Steps Taken: 14\n",
      "仿真完成，耗时 20.07 秒\n"
     ]
    }
   ],
   "source": [
    "grid_map = np.loadtxt(\"grid_map_final.txt\", dtype=int)\n",
    "nodes=[(51, 12), (55, 23), (55, 30), (32, 47), (9, 50), (9, 64)]\n",
    "env = GridWorld(grid_map , start=(58, 5) , goal=(51, 12),goal_list=nodes)\n",
    "#[(58, 5), (51, 12), (55, 23), (55, 30), (32, 47), (9, 50), (9, 64), (5, 69)]\n",
    "# 实例化 RainbowDQN\n",
    "agent = RainbowDQN(\n",
    "    env=env,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    learning_rate=5e-3,\n",
    "    batch_size=64,\n",
    "    memory_size=20000\n",
    ")\n",
    "check_trained_model(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    model_path=\"dqn_model.pth\",  # 替换为你的模型文件路径\n",
    "    max_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from dqn_model30_10_f.pth\n",
      "Loaded model from dqn_model30_10_f.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_34892\\3216142474.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Completed. Success Count: 646/1000\n"
     ]
    }
   ],
   "source": [
    "# 加载环境和模型\n",
    "grid_map = np.loadtxt(\"grid_map_final.txt\", dtype=int)\n",
    "env = GridWorld(grid_map , start=(58, 5), goal=(51, 12), goal_list=nodes)\n",
    "agent = RainbowDQN(\n",
    "    env=env,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    learning_rate=5e-3,\n",
    "    batch_size=64,\n",
    "    memory_size=20000\n",
    ")\n",
    "\n",
    "# 运行100次测试\n",
    "check_trained_model_num(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    model_path=\"dqn_model30_10_f.pth\",  # 替换为你的模型文件路径\n",
    "    max_steps=1000,\n",
    "    num_runs=1000  # 运行100次测试\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
