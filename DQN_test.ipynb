{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, grid_map, start, goal, max_steps=50000, goal_list=None, obs=30, dobs=10):\n",
    "        self.grid_map = grid_map\n",
    "        self.base_grid_map = grid_map.copy()\n",
    "        self.rows, self.cols = grid_map.shape\n",
    "        self.max_steps = max_steps\n",
    "        self.goal_list = goal_list if goal_list else [] \n",
    "        self.history = deque(maxlen=4) \n",
    "        self.num_obstacles = obs\n",
    "        self.num_dobs = dobs\n",
    "        \n",
    "        if start is not None and goal is not None:\n",
    "            self.start = start\n",
    "            self.goal = goal\n",
    "            self.agent_pos = self.start\n",
    "            self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "        else:\n",
    "            self.reset_dynamic(start, goal)\n",
    "\n",
    "    def reset_dynamic(self):\n",
    "        \"\"\"\n",
    "        每次 reset 时动态生成障碍物，并使用传入的起点和目标，或者随机生成。\n",
    "        \"\"\"\n",
    "        self.grid_map = self.base_grid_map.copy()\n",
    "        self.dynamic_obstacles = []\n",
    "        self.obstacle_history = {i: deque(maxlen=4) for i in range(self.num_dobs)}  # 每个障碍物的历史\n",
    "        # 固定障碍物初始化\n",
    "        valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        random_obstacles = random.sample(valid_positions, self.num_obstacles)\n",
    "        for x, y in random_obstacles:\n",
    "            self.grid_map[x, y] = 1\n",
    "            \n",
    "        for _ in range(self.num_dobs):\n",
    "            x, y = random.choice(valid_positions)\n",
    "            direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "            speed = random.randint(1, 2)\n",
    "            self.dynamic_obstacles.append({\n",
    "                'position': (x, y),\n",
    "                'direction': direction,\n",
    "                'speed': speed,\n",
    "                'history': deque([None]*4, maxlen=4)\n",
    "            })\n",
    "\n",
    "        # 确保起点和目标点不在障碍物中\n",
    "        self.valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        self.agent_pos = self.start\n",
    "        self.steps = 0\n",
    "        self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态\n",
    "        \"\"\"\n",
    "        self.reset_dynamic()\n",
    "        self.goal_list_copy = self.goal_list.copy()\n",
    "        self.current_goal_index = 0\n",
    "        self.goal = self.goal_list[self.current_goal_index] if self.goal_list else None\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self, n_frames=4):  \n",
    "        nearby_grid = np.ones((7, 7), dtype=int)  # 初始为1表示空白区域\n",
    "        x_min, x_max = max(0, self.agent_pos[0] - 3), min(self.rows, self.agent_pos[0] + 4)\n",
    "        y_min, y_max = max(0, self.agent_pos[1] - 3), min(self.cols, self.agent_pos[1] + 4)\n",
    "        r_min, r_max = 3 - (self.agent_pos[0] - x_min), 3 + (x_max - self.agent_pos[0])\n",
    "        c_min, c_max = 3 - (self.agent_pos[1] - y_min), 3 + (y_max - self.agent_pos[1])\n",
    "\n",
    "        # 获取当前智能体周围的区域\n",
    "        nearby_grid[r_min:r_max, c_min:c_max] = self.grid_map[x_min:x_max, y_min:y_max]\n",
    "        # nearby_flat = nearby_grid.flatten()\n",
    "\n",
    "        # 计算距离目标的距离和角度\n",
    "        dx = self.goal[0] - self.agent_pos[0]\n",
    "        dy = self.goal[1] - self.agent_pos[1]\n",
    "        distance_to_goal = np.sqrt(dx**2 + dy**2)\n",
    "        angle_to_goal = np.arctan2(dy, dx)\n",
    "\n",
    "        # current_state = np.concatenate(([distance_to_goal, angle_to_goal], nearby_flat))\n",
    "\n",
    "        # 添加动态障碍物的预测位置到状态\n",
    "        for idx, obstacle in enumerate(self.dynamic_obstacles):  # 遍历所有障碍物\n",
    "            self.obstacle_history[idx].append(obstacle['position'])\n",
    "            if len(self.obstacle_history[idx]) >= 4:\n",
    "                # 计算障碍物的速度（基于过去4帧）\n",
    "                pos_diff = np.array(self.obstacle_history[idx][-1]) - np.array(self.obstacle_history[idx][-2])\n",
    "                predicted_pos = np.array(obstacle['position']) + pos_diff  # 预测未来位置\n",
    "            else:\n",
    "                predicted_pos = np.array(obstacle['position'])  # 若历史帧数不足，使用当前位\n",
    "\n",
    "            # 将动态障碍物预测位置映射到7x7网格范围内\n",
    "            dx_obstacle = predicted_pos[0] - self.agent_pos[0]\n",
    "            dy_obstacle = predicted_pos[1] - self.agent_pos[1]\n",
    "\n",
    "            # 如果障碍物预测位置在检测范围内，则临时将该栅格设为3\n",
    "            if abs(dx_obstacle) <= 3 and abs(dy_obstacle) <= 3:\n",
    "                grid_x = int(dx_obstacle + 3)  # 偏移到0到6之间\n",
    "                grid_y = int(dy_obstacle + 3)  # 偏移到0到6之间\n",
    "                nearby_grid[grid_x, grid_y] = 3  # 将该栅格标记为动态障碍物的预测落点\n",
    "\n",
    "        # 展平并将动态障碍物的影响加入状态\n",
    "        nearby_flat_with_dynamic = nearby_grid.flatten()\n",
    "        current_state = np.concatenate(([distance_to_goal, angle_to_goal], nearby_flat_with_dynamic))\n",
    "\n",
    "        self.history.append(current_state)\n",
    "        while len(self.history) < n_frames:\n",
    "            self.history.appendleft(current_state)\n",
    "\n",
    "        return np.array(self.history)  # 返回(n_frames, state_dim)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作并更新环境状态\n",
    "        \"\"\"\n",
    "        state = self.get_state()\n",
    "        nearby_flat = state[-1][2:]\n",
    "\n",
    "        actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1), (0 , 0)]\n",
    "        delta = actions[action]\n",
    "        next_pos = (self.agent_pos[0] + delta[0], self.agent_pos[1] + delta[1])\n",
    "\n",
    "        # 检查是否越界或碰到障碍物（包括动态障碍物和固定障碍物）\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward = -5.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        if not (0 <= next_pos[0] < self.rows and 0 <= next_pos[1] < self.cols) or self.grid_map[next_pos] == 1:\n",
    "            reward = -5.0\n",
    "            done = True\n",
    "            return self.get_state(), reward, done\n",
    "\n",
    "        self.agent_pos = next_pos\n",
    "        reward = -0.5\n",
    "        done = False\n",
    "\n",
    "        next_distance = np.sqrt((self.goal[0] - self.agent_pos[0])**2 + (self.goal[1] - self.agent_pos[1])**2)\n",
    "        if(self.distance > next_distance):\n",
    "            reward += 0.6\n",
    "        else:\n",
    "            reward -= 0.4\n",
    "        self.distance = next_distance\n",
    "\n",
    "        action_vector = np.array([delta[0], delta[1]])\n",
    "        goal_vector = np.array([self.goal[0] - self.agent_pos[0], self.goal[1] - self.agent_pos[1]])\n",
    "        goal_vector_norm = goal_vector / (np.linalg.norm(goal_vector) + 1e-5)\n",
    "        \n",
    "        if np.linalg.norm(goal_vector) > 0.1:\n",
    "            alignment_reward = np.dot(action_vector, goal_vector_norm)\n",
    "        else:\n",
    "            alignment_reward = 0\n",
    "        \n",
    "        reward += alignment_reward * 0.4\n",
    "\n",
    "        if self.distance < 0.5:\n",
    "            reward += 10\n",
    "            if self.current_goal_index + 1 < len(self.goal_list):\n",
    "                self.current_goal_index += 1\n",
    "                self.goal = self.goal_list[self.current_goal_index]  \n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        # 更新动态障碍物的位置\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            x, y = obstacle['position']\n",
    "            direction = obstacle['direction']\n",
    "            speed = obstacle['speed']\n",
    "            \n",
    "            if direction == 'up':\n",
    "                new_pos = (x - speed, y)\n",
    "            elif direction == 'down':\n",
    "                new_pos = (x + speed, y)\n",
    "            elif direction == 'left':\n",
    "                new_pos = (x, y - speed)\n",
    "            elif direction == 'right':\n",
    "                new_pos = (x, y + speed)\n",
    "            \n",
    "            if 0 <= new_pos[0] < self.rows and 0 <= new_pos[1] < self.cols:\n",
    "                if self.grid_map[new_pos[0], new_pos[1]] == 1:\n",
    "                    if direction == 'up':\n",
    "                        obstacle['direction'] = 'down'\n",
    "                    elif direction == 'down':\n",
    "                        obstacle['direction'] = 'up'\n",
    "                    elif direction == 'left':\n",
    "                        obstacle['direction'] = 'right'\n",
    "                    elif direction == 'right':\n",
    "                        obstacle['direction'] = 'left'\n",
    "                else:\n",
    "                    # 更新障碍物的位置\n",
    "                    self.grid_map[x, y] = 0  # 清除旧位置\n",
    "                    self.grid_map[new_pos[0], new_pos[1]] = 1  # 设置新位置\n",
    "                    obstacle['position'] = new_pos\n",
    "\n",
    "        # 计算最小障碍物距离\n",
    "        min_distance_to_obstacle = float('inf')\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if self.grid_map[i, j] == 1:  # 障碍物\n",
    "                    distance_to_obstacle = np.linalg.norm(np.array(self.agent_pos) - np.array((i, j)))\n",
    "                    min_distance_to_obstacle = min(min_distance_to_obstacle, distance_to_obstacle)\n",
    "\n",
    "        # 奖励：距离障碍物越近，惩罚越大\n",
    "        if min_distance_to_obstacle == 1.0:\n",
    "            reward -= 2.0\n",
    "        elif min_distance_to_obstacle <= 2.0:\n",
    "            reward -= 1.5\n",
    "        elif min_distance_to_obstacle <= 3.0:\n",
    "            reward -= 0.2\n",
    "        else :\n",
    "            reward -= 0.01\n",
    "        \n",
    "        # 检查智能体与预测落点的距离\n",
    "        for idx, cell in enumerate(nearby_flat):\n",
    "            if cell == 3:  # 栅格标记为3，表示动态障碍物的预测落点\n",
    "                grid_x, grid_y = divmod(idx, 7)  # 还原栅格的坐标\n",
    "                # 将局部坐标转换为全局坐标\n",
    "                global_x = self.agent_pos[0] + (grid_x - 3)\n",
    "                global_y = self.agent_pos[1] + (grid_y - 3)\n",
    "                distance_to_prediction = np.linalg.norm(np.array(self.agent_pos) - np.array((global_x, global_y)))\n",
    "                if distance_to_prediction < 2:  # 距离预测落点过近\n",
    "                    reward -= 2.75  # 惩罚\n",
    "\n",
    "        # 检查是否碰到动态障碍物\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward -= 5.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "            reward -= 10\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN_with_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, lstm_layers=1):\n",
    "        super(DuelingDQN_with_LSTM, self).__init__()\n",
    "\n",
    "        # 输入数据的前两项是距离和角度信息\n",
    "        self.distance_angle_fc = nn.Linear(2, 64)  # 处理距离和角度的全连接层\n",
    "\n",
    "        # 栅格部分：7x7栅格图像的卷积处理\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)  # 7x7x16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)  # 4x4x32\n",
    "        \n",
    "        # LSTM输入维度 = 32*4*4 + 64（卷积输出特征 + 距离/角度特征）\n",
    "        self.lstm_input_dim = 32 * 4 * 4 + 64\n",
    "\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_dim, hidden_size=hidden_dim, num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        # 状态价值（V(s)）分支\n",
    "        self.value_fc = nn.Linear(hidden_dim, 128)\n",
    "        self.value_output = nn.Linear(128, 1)\n",
    "\n",
    "        # 优势（A(s, a)）分支\n",
    "        self.advantage_fc = nn.Linear(hidden_dim, 128)\n",
    "        self.advantage_output = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, state_dim = x.size()\n",
    "\n",
    "        # 提取栅格部分：假设栅格信息位于第三维及以后的部分\n",
    "        grid_states = x[:, :, 2:].view(batch_size * time_steps, 1, 7, 7)\n",
    "\n",
    "        # 卷积处理栅格数据\n",
    "        grid_features = F.relu(self.conv1(grid_states))\n",
    "        grid_features = F.relu(self.conv2(grid_features))\n",
    "\n",
    "        # 展平卷积层输出\n",
    "        grid_features = grid_features.view(batch_size * time_steps, -1)\n",
    "\n",
    "        # 提取距离和角度信息：前两项数据\n",
    "        distance_angle = x[:, :, :2].view(batch_size * time_steps, 2)\n",
    "        distance_angle = F.relu(self.distance_angle_fc(distance_angle))  # 经过全连接层\n",
    "\n",
    "        # 将距离和角度信息与卷积输出拼接\n",
    "        combined_features = torch.cat([grid_features, distance_angle], dim=1)\n",
    "\n",
    "        # LSTM处理\n",
    "        combined_features = combined_features.view(batch_size, time_steps, -1)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(combined_features)\n",
    "\n",
    "        # 使用最后一个时间步的输出\n",
    "        lstm_out_last = lstm_out[:, -1, :]\n",
    "\n",
    "        value = F.relu(self.value_fc(lstm_out_last))\n",
    "        value = self.value_output(value)\n",
    "\n",
    "        advantage = F.relu(self.advantage_fc(lstm_out_last))\n",
    "        advantage = self.advantage_output(advantage)\n",
    "\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN:\n",
    "    def __init__(self, env, gamma=0.8, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9, \n",
    "                learning_rate=4e-4, batch_size=64, memory_size=10000, device=None, alpha=0.6, beta=0.4,\n",
    "                n_step=4, shared_replay_buffer=None):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha  # 优先级的重要性\n",
    "        self.beta = beta    # 用于优先级采样的偏差修正\n",
    "        self.n_step = n_step  # Multi-step 参数\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 使用 Dueling DQN 网络\n",
    "        self.q_network = DuelingDQN_with_LSTM(51, 9).to(self.device)  # 输入51是特征维度，输出9是动作维度\n",
    "        self.target_network = DuelingDQN_with_LSTM(51, 9).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        # 使用共享缓冲区（如果提供），否则实例化私有缓冲区\n",
    "        self.memory = shared_replay_buffer or deque(maxlen=memory_size)\n",
    "        self.priority_sum = 0  # 初始化总优先级为 0\n",
    "        self.update_target_network()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, 8)\n",
    "        \n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        state_tensor = state_tensor.unsqueeze(0)  # (1, n_frames, state_dim) 转换为三维张量\n",
    "        \n",
    "        q_values = self.q_network(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        # 计算多步TD误差\n",
    "        td_error = 0.0  # 初始时，TD error 为 0\n",
    "\n",
    "        if len(self.memory) > 0:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0)  # (1, n_frames, state_dim)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device).unsqueeze(0)  # (1, n_frames, state_dim)\n",
    "            \n",
    "            # 计算 Q 值和 TD Error\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            action_tensor = torch.tensor([action], dtype=torch.long).to(self.device)\n",
    "            current_q_values = q_values.gather(1, action_tensor.view(-1, 1)).squeeze()\n",
    "            next_q_values = self.target_network(next_state_tensor).max()\n",
    "            td_error = abs(reward + self.gamma * next_q_values.item() - current_q_values.item())\n",
    "        \n",
    "        # 存储经验并附加优先级\n",
    "        priority = (td_error + 1e-5) ** self.alpha  # 1e-5 防止优先级为零\n",
    "        self.memory.append((state, action, reward, next_state, done, priority))\n",
    "        self.priority_sum += priority\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # 使用均匀采样\n",
    "        #indices = random.sample(range(len(self.memory)), self.batch_size)\n",
    "        #batch = [self.memory[idx] for idx in indices]\n",
    "        # 使用优先级采样\n",
    "        probabilities = np.array([transition[5] for transition in self.memory])\n",
    "        probabilities += 1e-5  # 防止某些优先级为零\n",
    "        probabilities /= probabilities.sum()  # 归一化概率总和为1\n",
    "        indices = np.random.choice(len(self.memory), self.batch_size, p=probabilities)\n",
    "        batch = [self.memory[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones, priorities = zip(*batch)\n",
    "        \n",
    "        # 转换为 Tensor\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        if states.dim() == 2:\n",
    "            states = states.unsqueeze(0)  # (batch_size, n_frames, state_dim)\n",
    "        if next_states.dim() == 2:\n",
    "            next_states = next_states.unsqueeze(0)  # (batch_size, n_frames, state_dim)\n",
    "\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # 计算多步奖励目标\n",
    "        target_q_values = rewards\n",
    "        for i in range(1, self.n_step):\n",
    "            target_q_values += (self.gamma ** i) * rewards\n",
    "\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        target_q_values += (self.gamma ** self.n_step) * next_q_values * (1 - dones)\n",
    "\n",
    "        # 计算损失\n",
    "        current_q_values = self.q_network(states).gather(1, actions.view(-1, 1)).squeeze()\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        # 反向传播\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 更新目标网络\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update(self):\n",
    "        self.update_target_network()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, file_path=\"dqn_model.pth\"):\n",
    "        torch.save({\n",
    "            'q_network': self.q_network.state_dict(),\n",
    "            'target_network': self.target_network.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "    \n",
    "    def load_model(self, file_path=\"dqn_model.pth\"):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        print(f\"Model loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(env, agent, episodes=1000, max_steps=1000, save_path=\"dqn_model.pth\"):\n",
    "    import pygame\n",
    "    import numpy as np\n",
    "\n",
    "    pygame.init()\n",
    "    grid_size = 10  # 每个格子的像素大小\n",
    "    screen = pygame.display.set_mode((env.cols * grid_size, env.rows * grid_size))\n",
    "    pygame.display.set_caption(\"Training Visualization\")\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    pygame.font.init()\n",
    "    font = pygame.font.SysFont(\"Arial\", 24)\n",
    "\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        env.reset_dynamic(num_obstacles=min(5 + episode // 100, 20))  # 动态调整障碍物数量\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        path = [env.agent_pos]  # 路径记录\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # 渲染环境\n",
    "            screen.fill((255, 255, 255))  # 清空屏幕，设置白色背景\n",
    "\n",
    "            # 绘制栅格地图\n",
    "            for i in range(env.rows):\n",
    "                for j in range(env.cols):\n",
    "                    color = (255, 255, 255)  # 默认白色\n",
    "                    if env.grid_map[i, j] == 1:\n",
    "                        color = (0, 0, 0)  # 黑色障碍物\n",
    "                    pygame.draw.rect(screen, color, (j * grid_size, i * grid_size, grid_size, grid_size))\n",
    "\n",
    "            # 绘制目标\n",
    "            pygame.draw.rect(\n",
    "                screen,\n",
    "                (0, 0, 255),\n",
    "                (env.goal[1] * grid_size, env.goal[0] * grid_size, grid_size, grid_size)\n",
    "            )  # 蓝色目标点\n",
    "\n",
    "            # 绘制路径轨迹\n",
    "            for pos in path:\n",
    "                pygame.draw.circle(\n",
    "                    screen,\n",
    "                    (200, 200, 200),  # 浅灰色轨迹\n",
    "                    (int(pos[1] * grid_size + grid_size // 2), int(pos[0] * grid_size + grid_size // 2)),\n",
    "                    5\n",
    "                )\n",
    "\n",
    "            # 绘制智能体\n",
    "            car_x = int(env.agent_pos[1] * grid_size + grid_size // 2)\n",
    "            car_y = int(env.agent_pos[0] * grid_size + grid_size // 2)\n",
    "            car_radius = grid_size // 4\n",
    "            pygame.draw.circle(screen, (255, 0, 0), (car_x, car_y), car_radius)\n",
    "\n",
    "            # 智能体选择动作\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(agent.device)\n",
    "            action = agent.select_action(state_tensor)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "\n",
    "            # 更新状态和累计奖励\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path.append(env.agent_pos)\n",
    "            steps += 1\n",
    "\n",
    "            # 显示单步奖励\n",
    "            reward_text = font.render(f\"Episode: {episode}\", True, (0, 0, 0))\n",
    "            screen.blit(reward_text, (10, 10))\n",
    "\n",
    "            # 显示总奖励\n",
    "            total_reward_text = font.render(f\"Total Reward: {total_reward:.2f}\", True, (0, 0, 0))\n",
    "            screen.blit(total_reward_text, (10, 40))\n",
    "\n",
    "            pygame.display.flip()  # 更新显示\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "            clock.tick(60)\n",
    "\n",
    "        # 记录最佳模型\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            agent.save_model(save_path)\n",
    "\n",
    "        # 每 10 轮打印日志\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}: Total Reward = {total_reward:.2f}\")\n",
    "\n",
    "    pygame.quit()\n",
    "    print(\"Training Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=1000, max_steps=1000, save_path=\"dqn_model.pth\", log_dir=\"runs\"): \n",
    "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = os.path.join(log_dir, f\"run_{run_id}\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # 添加网络模型到 TensorBoard（仅记录一次）\n",
    "    dummy_input = torch.rand(1, 10, 51).to(agent.device)\n",
    "    writer.add_graph(agent.q_network, dummy_input)  # 将模型结构写入 TensorBoard\n",
    "\n",
    "    best_reward = -float('inf')  # 记录最佳奖励\n",
    "    step_counter = 0  # 全局步数计数器\n",
    "    all_rewards = []  # 保存所有回合奖励\n",
    "    all_losses = []  # 保存所有回合平均损失\n",
    "    all_epsilons = []  # 保存所有回合探索率\n",
    "\n",
    "    # 使用 tqdm 进度条显示训练过程\n",
    "    with tqdm(total=episodes, desc=\"训练进度\", unit=\"episode\") as pbar:\n",
    "        for episode in range(episodes):\n",
    "            env.reset_dynamic()  # 重置环境动态障碍物\n",
    "            state = env.reset()  # 获取初始状态\n",
    "            done = False\n",
    "            total_reward = 0  # 当前回合总奖励\n",
    "            episode_steps = 0  # 当前回合步数计数\n",
    "            losses = []  # 当前回合的损失列表\n",
    "            q_values = []  # 当前回合的 Q 值记录\n",
    "\n",
    "            # 在每个回合内训练多个目标点之间的路径\n",
    "            goal_pairs = zip(env.goal_list, env.goal_list[1:])\n",
    "            for start_goal, next_goal in goal_pairs:\n",
    "                env.goal = start_goal  # 设置当前目标\n",
    "                while not done:\n",
    "                    # 转换状态为 Tensor\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).to(agent.device)\n",
    "                    \n",
    "                    # 智能体选择动作\n",
    "                    action = agent.select_action(state_tensor)\n",
    "                    \n",
    "                    # 环境执行动作并返回新状态、奖励和是否结束\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    \n",
    "                    # 存储经验和训练模型\n",
    "                    agent.store_transition(state, action, reward, next_state, done)\n",
    "                    loss = agent.train()  # 返回当前训练的损失\n",
    "                    state = next_state\n",
    "                    total_reward += reward\n",
    "                    episode_steps += 1\n",
    "                    step_counter += 1\n",
    "\n",
    "                    # 记录损失和 Q 值分布\n",
    "                    if loss is not None:\n",
    "                        losses.append(loss.item())\n",
    "                    with torch.no_grad():\n",
    "                        q_values.append(agent.q_network(state_tensor.unsqueeze(0)).cpu().numpy().flatten())\n",
    "\n",
    "                    # 如果达到最大步数，则结束当前回合\n",
    "                    if episode_steps >= max_steps:\n",
    "                        done = True\n",
    "\n",
    "            # 每个回合结束后，衰减 epsilon（探索率）\n",
    "            agent.decay_epsilon()\n",
    "\n",
    "            # 记录奖励、epsilon 和损失到 TensorBoard\n",
    "            writer.add_scalar(\"Reward/Episode\", total_reward, episode)\n",
    "            writer.add_scalar(\"Epsilon\", agent.epsilon, episode)\n",
    "            if losses:\n",
    "                avg_loss = np.mean(losses)\n",
    "                writer.add_scalar(\"Loss/Episode\", avg_loss, episode)\n",
    "                all_losses.append(avg_loss)\n",
    "            all_rewards.append(total_reward)\n",
    "            all_epsilons.append(agent.epsilon)\n",
    "\n",
    "            # 记录 Q 值分布\n",
    "            if q_values:\n",
    "                q_values_flat = np.concatenate(q_values)\n",
    "                writer.add_histogram(\"Q_values/Distribution\", q_values_flat, episode)\n",
    "\n",
    "            # 更新 tqdm 进度条信息\n",
    "            pbar.set_postfix({\"总奖励\": total_reward, \"Epsilon\": round(agent.epsilon, 3)})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # 如果当前回合的奖励是历史最高，则保存模型\n",
    "            if total_reward > best_reward:\n",
    "                best_reward = total_reward\n",
    "                agent.save_model(save_path)\n",
    "\n",
    "            # 每 10 个回合刷新 TensorBoard 数据\n",
    "            if episode % 10 == 0:\n",
    "                writer.flush()\n",
    "\n",
    "        # 保存最终模型\n",
    "        agent.save_model(save_path)\n",
    "\n",
    "        # 关闭 TensorBoard 日志记录器\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"训练完成，模型已保存到 {save_path}\")\n",
    "\n",
    "    # 保存奖励、损失和 epsilon 数据以便后续绘图\n",
    "    np.save(os.path.join(log_dir, \"rewards.npy\"), all_rewards)\n",
    "    np.save(os.path.join(log_dir, \"losses.npy\"), all_losses)\n",
    "    np.save(os.path.join(log_dir, \"epsilons.npy\"), all_epsilons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trained_model_num(env, agent, model_path, max_steps=1000, num_runs=100, result_file=\"./test/test_results.txt\"):\n",
    "    # 加载训练好的模型\n",
    "    agent.load_model(model_path)\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "    # 设置 epsilon 为 0（测试时只选择最优动作）\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    success_count = 0  # 记录成功次数\n",
    "\n",
    "    # 打开文件进行写入\n",
    "    with open(result_file, 'w') as f:\n",
    "        # 写入文件头\n",
    "        f.write(\"Test Results for Trained Model\\n\")\n",
    "        f.write(f\"Model Path: {model_path}\\n\")\n",
    "        f.write(f\"Max Steps: {max_steps}, Total Runs: {num_runs}\\n\\n\")\n",
    "        f.write(\"Run\\tSuccess\\n\")\n",
    "        \n",
    "        # 测试多次\n",
    "        for run in range(num_runs):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            total_reward = 0\n",
    "            path = [env.agent_pos]  # 路径记录\n",
    "\n",
    "            while not done and steps < max_steps:\n",
    "                # 智能体选择动作\n",
    "                action = agent.select_action(state)  # 测试时选择最优动作\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                # 更新状态和累计奖励\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                path.append(env.agent_pos)  # 记录路径\n",
    "                steps += 1\n",
    "\n",
    "                # 检查是否到达目标\n",
    "                if env.agent_pos == env.goal:\n",
    "                    success_count += 1\n",
    "                    # 写入测试成功的结果\n",
    "                    f.write(f\"{run+1}\\tSuccess\\n\")\n",
    "                    break  # 达到目标则结束当前测试\n",
    "            else:\n",
    "                # 如果没有成功，到达最大步数\n",
    "                f.write(f\"{run+1}\\tFailed\\n\")\n",
    "        \n",
    "        # 输出成功次数\n",
    "        print(f\"Test Completed. Success Count: {success_count}/{num_runs}\")\n",
    "        # 将最终的成功率写入文件\n",
    "        f.write(f\"\\nTotal Success Count: {success_count}/{num_runs}\\n\")\n",
    "        f.write(f\"Success Rate: {success_count / num_runs * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trained_model(env, agent, model_path, max_steps=1000): \n",
    "    # 加载训练好的模型\n",
    "    agent.load_model(model_path)\n",
    "    start_time = time.time()\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "    # 设置 epsilon 为 0（测试时只选择最优动作）\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    # 初始化 Pygame\n",
    "    pygame.init()\n",
    "    grid_size = 10  # 每个格子的像素大小\n",
    "    screen = pygame.display.set_mode((env.cols * grid_size, env.rows * grid_size))\n",
    "    pygame.display.set_caption(\"Trained Model Test\")\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    # 初始化字体\n",
    "    pygame.font.init()\n",
    "    font = pygame.font.SysFont(\"Arial\", 24)\n",
    "\n",
    "    # 测试\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    path = [env.agent_pos]  # 路径记录\n",
    "    steps = 0\n",
    "    \n",
    "    # 标记哪些格子被点击并变为红色\n",
    "    clicked_cells = set()\n",
    "\n",
    "    # 动态障碍物路径记录\n",
    "    for obstacle in env.dynamic_obstacles:\n",
    "        obstacle['path'] = [tuple(obstacle['position'])]  # 初始化路径记录\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        # 渲染环境\n",
    "        screen.fill((255, 255, 255))  # 清空屏幕，设置白色背景\n",
    "        dynamic_positions = set([tuple(obstacle['position']) for obstacle in env.dynamic_obstacles])\n",
    "\n",
    "        # 绘制栅格地图\n",
    "        for i in range(env.rows):\n",
    "            for j in range(env.cols):\n",
    "                color = (255, 255, 255)  # 默认白色\n",
    "                if env.grid_map[i, j] == 1:\n",
    "                    if (i, j) in clicked_cells:\n",
    "                        color = (255, 0, 0)  # 点击的格子为红色\n",
    "                    elif (i, j) in dynamic_positions:\n",
    "                        color = (210, 180, 140)  # 动态障碍物为土黄色\n",
    "                    else:\n",
    "                        color = (0, 0, 0)  # 黑色障碍物\n",
    "                pygame.draw.rect(screen, color, (j * grid_size, i * grid_size, grid_size, grid_size))\n",
    "\n",
    "        # 绘制动态障碍物的运动轨迹\n",
    "        for obstacle in env.dynamic_obstacles:\n",
    "            for prev_pos in obstacle['path']:\n",
    "                prev_x, prev_y = prev_pos\n",
    "                pygame.draw.circle(screen, (210, 180, 140), (prev_y * grid_size + grid_size // 2, prev_x * grid_size + grid_size // 2), 2)\n",
    "\n",
    "        # 绘制目标\n",
    "        pygame.draw.rect(\n",
    "            screen,\n",
    "            (0, 0, 255),\n",
    "            (env.goal[1] * grid_size, env.goal[0] * grid_size, grid_size, grid_size)\n",
    "        )  # 蓝色目标点\n",
    "\n",
    "        # 绘制小车的路径轨迹\n",
    "        for pos in path:\n",
    "            pygame.draw.circle(screen, (200, 200, 200), (int(pos[1] * grid_size + grid_size // 2), int(pos[0] * grid_size + grid_size // 2)), 2)\n",
    "\n",
    "        # 绘制智能体\n",
    "        car_x = int(env.agent_pos[1] * grid_size + grid_size // 2)\n",
    "        car_y = int(env.agent_pos[0] * grid_size + grid_size // 2)\n",
    "        car_radius = grid_size // 4\n",
    "\n",
    "        # 绘制小车的主体\n",
    "        pygame.draw.circle(screen, (255, 0, 0), (car_x, car_y), car_radius)\n",
    "\n",
    "        # 智能体选择动作\n",
    "        action = agent.select_action(state)  # 测试时选择最优动作\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # 更新动态障碍物的路径\n",
    "        for obstacle in env.dynamic_obstacles:\n",
    "            obstacle['path'].append(tuple(obstacle['position']))\n",
    "\n",
    "        pygame.display.flip()  # 更新显示\n",
    "\n",
    "        # 更新状态和累计奖励\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        path.append(env.agent_pos)  # 记录路径\n",
    "        steps += 1\n",
    "\n",
    "        # 检查是否按下退出事件或者添加障碍物\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "            elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                # 获取鼠标点击的坐标\n",
    "                mouse_x, mouse_y = pygame.mouse.get_pos()\n",
    "                # 转换为对应的栅格坐标\n",
    "                grid_x = mouse_x // grid_size\n",
    "                grid_y = mouse_y // grid_size\n",
    "                # 在地图中添加障碍物\n",
    "                if env.grid_map[grid_y, grid_x] == 0:  # 只在空白位置添加障碍物\n",
    "                    env.grid_map[grid_y, grid_x] = 1\n",
    "                    clicked_cells.add((grid_y, grid_x))\n",
    "                    print(f\"Obstacle added at ({grid_y}, {grid_x})\")\n",
    "        \n",
    "        # 限制帧率\n",
    "        clock.tick(1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    pygame.quit()  # 测试完成后关闭窗口\n",
    "\n",
    "    # 输出测试结果\n",
    "    print(f\"Test Completed. Total Reward: {total_reward}, Steps Taken: {steps}\")\n",
    "    print(f\"仿真完成，耗时 {end_time - start_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"grid_map_final.txt\"\n",
    "grid_map = np.loadtxt(files, dtype=int)\n",
    "obst = [[0,0],\n",
    "        [30,0],\n",
    "        [0,10],\n",
    "        [0,30],\n",
    "        [30,10],\n",
    "        [30,20],\n",
    "        [30,30]]\n",
    "nodes = [(58, 5), (51, 12), (55, 23), (55, 30), (32, 47), (9, 50), (9, 64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 5/1000 [00:00<01:40,  9.92episode/s, 总奖励=1.13, Epsilon=0.97]C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15088\\3824810451.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   1%|▏         | 13/1000 [00:00<00:47, 20.94episode/s, 总奖励=-7.3, Epsilon=0.932] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   4%|▍         | 38/1000 [00:05<02:34,  6.24episode/s, 总奖励=-23.5, Epsilon=0.822]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  12%|█▏        | 122/1000 [00:51<12:04,  1.21episode/s, 总奖励=-11, Epsilon=0.543]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  13%|█▎        | 134/1000 [01:14<28:23,  1.97s/episode, 总奖励=-5, Epsilon=0.508]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  15%|█▍        | 147/1000 [01:31<18:06,  1.27s/episode, 总奖励=-5, Epsilon=0.476]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  17%|█▋        | 169/1000 [02:13<27:49,  2.01s/episode, 总奖励=71.9, Epsilon=0.429]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  18%|█▊        | 176/1000 [02:22<14:46,  1.08s/episode, 总奖励=-7.3, Epsilon=0.414] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  18%|█▊        | 182/1000 [02:35<26:17,  1.93s/episode, 总奖励=82.7, Epsilon=0.402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  20%|██        | 201/1000 [03:20<33:39,  2.53s/episode, 总奖励=84, Epsilon=0.365]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  21%|██        | 207/1000 [03:31<21:35,  1.63s/episode, 总奖励=84.9, Epsilon=0.354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  27%|██▋       | 271/1000 [05:27<19:46,  1.63s/episode, 总奖励=87.5, Epsilon=0.257]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  28%|██▊       | 281/1000 [05:45<20:32,  1.71s/episode, 总奖励=90.6, Epsilon=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  29%|██▉       | 292/1000 [06:06<22:02,  1.87s/episode, 总奖励=-5, Epsilon=0.23]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  33%|███▎      | 331/1000 [07:11<12:23,  1.11s/episode, 总奖励=-0.101, Epsilon=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  35%|███▌      | 352/1000 [07:47<16:03,  1.49s/episode, 总奖励=98.2, Epsilon=0.171]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  41%|████      | 412/1000 [09:37<15:20,  1.57s/episode, 总奖励=98.5, Epsilon=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  43%|████▎     | 429/1000 [10:08<15:08,  1.59s/episode, 总奖励=99, Epsilon=0.116]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  49%|████▊     | 487/1000 [11:59<15:19,  1.79s/episode, 总奖励=99.4, Epsilon=0.087]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  49%|████▉     | 489/1000 [12:03<16:15,  1.91s/episode, 总奖励=101, Epsilon=0.086] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  50%|█████     | 504/1000 [12:33<13:00,  1.57s/episode, 总奖励=101, Epsilon=0.08]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  57%|█████▋    | 572/1000 [15:29<35:46,  5.02s/episode, 总奖励=102, Epsilon=0.057] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  66%|██████▌   | 656/1000 [22:39<29:29,  5.14s/episode, 总奖励=102, Epsilon=0.037] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  79%|███████▊  | 786/1000 [28:25<07:18,  2.05s/episode, 总奖励=102, Epsilon=0.019] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  85%|████████▌ | 850/1000 [30:35<04:41,  1.88s/episode, 总奖励=102, Epsilon=0.014] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  90%|████████▉ | 895/1000 [32:09<03:34,  2.04s/episode, 总奖励=102, Epsilon=0.011] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  91%|█████████ | 909/1000 [32:38<03:17,  2.17s/episode, 总奖励=102, Epsilon=0.01]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 1000/1000 [35:54<00:00,  2.15s/episode, 总奖励=102, Epsilon=0.007]\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15088\\3824810451.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "训练完成，模型已保存到 dqn_model.pth\n",
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Success Count: 1000/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 1/1000 [00:00<01:05, 15.36episode/s, 总奖励=0.8, Epsilon=0.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   4%|▍         | 41/1000 [00:06<01:42,  9.39episode/s, 总奖励=-5, Epsilon=0.81]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  15%|█▍        | 148/1000 [01:05<09:45,  1.45episode/s, 总奖励=23.8, Epsilon=0.476] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  15%|█▌        | 150/1000 [01:09<16:43,  1.18s/episode, 总奖励=35.6, Epsilon=0.471] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  16%|█▋        | 164/1000 [01:24<13:01,  1.07episode/s, 总奖励=48.6, Epsilon=0.44]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  17%|█▋        | 168/1000 [01:30<16:53,  1.22s/episode, 总奖励=-5, Epsilon=0.429]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  20%|█▉        | 198/1000 [01:59<18:13,  1.36s/episode, 总奖励=69.6, Epsilon=0.371] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  23%|██▎       | 233/1000 [02:47<21:35,  1.69s/episode, 总奖励=75, Epsilon=0.311]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  25%|██▌       | 253/1000 [03:17<16:28,  1.32s/episode, 总奖励=75.3, Epsilon=0.281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  26%|██▌       | 256/1000 [03:24<23:03,  1.86s/episode, 总奖励=80, Epsilon=0.277]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  26%|██▌       | 257/1000 [03:26<23:55,  1.93s/episode, 总奖励=80.9, Epsilon=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  27%|██▋       | 273/1000 [03:55<22:08,  1.83s/episode, 总奖励=83.1, Epsilon=0.255]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  29%|██▊       | 287/1000 [04:18<20:28,  1.72s/episode, 总奖励=-5, Epsilon=0.236]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  30%|███       | 302/1000 [04:32<16:15,  1.40s/episode, 总奖励=92, Epsilon=0.22]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  30%|███       | 303/1000 [04:34<17:33,  1.51s/episode, 总奖励=93.9, Epsilon=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  39%|███▉      | 388/1000 [07:03<19:34,  1.92s/episode, 总奖励=99.2, Epsilon=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  56%|█████▌    | 557/1000 [12:22<15:15,  2.07s/episode, 总奖励=99.7, Epsilon=0.061] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  65%|██████▌   | 654/1000 [15:36<11:39,  2.02s/episode, 总奖励=101, Epsilon=0.038] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  71%|███████   | 706/1000 [17:23<10:02,  2.05s/episode, 总奖励=101, Epsilon=0.029]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  84%|████████▍ | 840/1000 [21:55<05:37,  2.11s/episode, 总奖励=101, Epsilon=0.015] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 1000/1000 [27:43<00:00,  1.66s/episode, 总奖励=88.7, Epsilon=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "训练完成，模型已保存到 dqn_model.pth\n",
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Success Count: 940/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 0/1000 [00:00<?, ?episode/s, 总奖励=-7.3, Epsilon=0.995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 5/1000 [00:00<00:44, 22.22episode/s, 总奖励=-5, Epsilon=0.97]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   4%|▍         | 38/1000 [00:04<01:19, 12.06episode/s, 总奖励=-12.9, Epsilon=0.822]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  10%|▉         | 98/1000 [00:26<08:35,  1.75episode/s, 总奖励=-5, Epsilon=0.609]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  13%|█▎        | 126/1000 [00:46<20:15,  1.39s/episode, 总奖励=16.3, Epsilon=0.532]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  14%|█▎        | 136/1000 [00:53<12:39,  1.14episode/s, 总奖励=3.1, Epsilon=0.503]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  15%|█▌        | 151/1000 [01:07<13:35,  1.04episode/s, 总奖励=3.1, Epsilon=0.467]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  15%|█▌        | 154/1000 [01:08<10:57,  1.29episode/s, 总奖励=-5, Epsilon=0.46]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  17%|█▋        | 172/1000 [01:29<24:51,  1.80s/episode, 总奖励=62, Epsilon=0.422]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  19%|█▉        | 192/1000 [01:48<12:38,  1.07episode/s, 总奖励=-5, Epsilon=0.382]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  20%|██        | 201/1000 [02:00<18:47,  1.41s/episode, 总奖励=-4.43, Epsilon=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  23%|██▎       | 226/1000 [02:28<18:23,  1.43s/episode, 总奖励=80.8, Epsilon=0.322] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  28%|██▊       | 281/1000 [03:42<16:49,  1.40s/episode, 总奖励=10.3, Epsilon=0.243]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  32%|███▏      | 316/1000 [04:34<20:42,  1.82s/episode, 总奖励=92.3, Epsilon=0.205] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  35%|███▍      | 349/1000 [05:26<20:49,  1.92s/episode, 总奖励=93, Epsilon=0.174]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  38%|███▊      | 375/1000 [06:06<14:16,  1.37s/episode, 总奖励=3.1, Epsilon=0.152] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  41%|████      | 409/1000 [07:01<16:43,  1.70s/episode, 总奖励=96, Epsilon=0.129]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  45%|████▍     | 446/1000 [08:04<16:01,  1.74s/episode, 总奖励=98.1, Epsilon=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  46%|████▌     | 459/1000 [08:25<13:54,  1.54s/episode, 总奖励=99.8, Epsilon=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  58%|█████▊    | 577/1000 [11:44<12:09,  1.72s/episode, 总奖励=100, Epsilon=0.055]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  59%|█████▉    | 590/1000 [12:07<12:18,  1.80s/episode, 总奖励=101, Epsilon=0.052] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  61%|██████    | 610/1000 [12:39<11:19,  1.74s/episode, 总奖励=101, Epsilon=0.047] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  72%|███████▏  | 717/1000 [15:54<08:52,  1.88s/episode, 总奖励=101, Epsilon=0.027] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  74%|███████▎  | 736/1000 [16:27<08:17,  1.89s/episode, 总奖励=102, Epsilon=0.025] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 1000/1000 [25:34<00:00,  1.53s/episode, 总奖励=69.7, Epsilon=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "训练完成，模型已保存到 dqn_model.pth\n",
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Success Count: 908/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   1%|          | 6/1000 [00:00<00:49, 20.16episode/s, 总奖励=-5, Epsilon=0.966]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   1%|          | 11/1000 [00:00<01:13, 13.39episode/s, 总奖励=3.1, Epsilon=0.946] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   6%|▌         | 59/1000 [00:05<01:07, 13.93episode/s, 总奖励=-5, Epsilon=0.74]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  25%|██▌       | 253/1000 [00:46<09:58,  1.25episode/s, 总奖励=4.52, Epsilon=0.281]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  26%|██▌       | 261/1000 [00:51<07:06,  1.73episode/s, 总奖励=10.9, Epsilon=0.27]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  29%|██▉       | 288/1000 [01:13<06:42,  1.77episode/s, 总奖励=1.8, Epsilon=0.236]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  32%|███▏      | 321/1000 [01:38<12:59,  1.15s/episode, 总奖励=13.1, Epsilon=0.2]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  32%|███▏      | 322/1000 [01:39<11:30,  1.02s/episode, 总奖励=13.6, Epsilon=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  32%|███▎      | 325/1000 [01:41<08:21,  1.34episode/s, 总奖励=14.3, Epsilon=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  33%|███▎      | 328/1000 [01:44<09:11,  1.22episode/s, 总奖励=19.8, Epsilon=0.193] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  36%|███▌      | 356/1000 [01:59<08:17,  1.29episode/s, 总奖励=28.6, Epsilon=0.168]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  38%|███▊      | 378/1000 [02:17<07:52,  1.32episode/s, 总奖励=35.3, Epsilon=0.15]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  41%|████      | 407/1000 [02:44<10:42,  1.08s/episode, 总奖励=35.6, Epsilon=0.13]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  41%|████      | 410/1000 [02:47<09:35,  1.02episode/s, 总奖励=53.9, Epsilon=0.128]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  43%|████▎     | 429/1000 [03:09<16:37,  1.75s/episode, 总奖励=72.9, Epsilon=0.116]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  47%|████▋     | 466/1000 [03:55<12:44,  1.43s/episode, 总奖励=73.2, Epsilon=0.097]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  47%|████▋     | 474/1000 [04:08<15:43,  1.79s/episode, 总奖励=78.3, Epsilon=0.093] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  49%|████▉     | 493/1000 [04:36<14:38,  1.73s/episode, 总奖励=94.6, Epsilon=0.084]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  57%|█████▋    | 573/1000 [06:24<08:21,  1.18s/episode, 总奖励=96.3, Epsilon=0.057]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  71%|███████   | 706/1000 [09:44<08:31,  1.74s/episode, 总奖励=96.8, Epsilon=0.029]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  73%|███████▎  | 733/1000 [10:27<08:36,  1.93s/episode, 总奖励=98.2, Epsilon=0.025]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  78%|███████▊  | 781/1000 [11:44<05:38,  1.54s/episode, 总奖励=98.5, Epsilon=0.02] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  79%|███████▉  | 791/1000 [12:00<05:02,  1.45s/episode, 总奖励=100, Epsilon=0.019] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  91%|█████████ | 908/1000 [15:22<01:54,  1.25s/episode, 总奖励=102, Epsilon=0.011]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 1000/1000 [18:12<00:00,  1.09s/episode, 总奖励=92.4, Epsilon=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "训练完成，模型已保存到 dqn_model.pth\n",
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Success Count: 688/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 2/1000 [00:00<00:30, 33.21episode/s, 总奖励=3.1, Epsilon=0.985]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  17%|█▋        | 169/1000 [01:18<03:59,  3.48episode/s, 总奖励=-5, Epsilon=0.427]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  18%|█▊        | 178/1000 [01:26<10:28,  1.31episode/s, 总奖励=15.7, Epsilon=0.41]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  18%|█▊        | 179/1000 [01:27<10:42,  1.28episode/s, 总奖励=24.5, Epsilon=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  20%|█▉        | 196/1000 [01:42<10:12,  1.31episode/s, 总奖励=46.9, Epsilon=0.374]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  22%|██▏       | 218/1000 [02:06<10:04,  1.29episode/s, 总奖励=59, Epsilon=0.335]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  22%|██▏       | 220/1000 [02:10<17:16,  1.33s/episode, 总奖励=65.9, Epsilon=0.332] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  28%|██▊       | 277/1000 [03:21<16:49,  1.40s/episode, 总奖励=3.1, Epsilon=0.248]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  29%|██▉       | 291/1000 [03:40<17:01,  1.44s/episode, 总奖励=68.7, Epsilon=0.233]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  30%|███       | 302/1000 [03:58<18:53,  1.62s/episode, 总奖励=72.9, Epsilon=0.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  32%|███▏      | 315/1000 [04:18<20:10,  1.77s/episode, 总奖励=79, Epsilon=0.206]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  34%|███▎      | 336/1000 [04:46<12:09,  1.10s/episode, 总奖励=3.1, Epsilon=0.185]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  36%|███▋      | 364/1000 [05:33<20:12,  1.91s/episode, 总奖励=83.9, Epsilon=0.161] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  39%|███▉      | 388/1000 [06:11<18:37,  1.83s/episode, 总奖励=3.1, Epsilon=0.142] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  41%|████      | 412/1000 [06:44<16:27,  1.68s/episode, 总奖励=-5.88, Epsilon=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  42%|████▏     | 421/1000 [06:54<13:32,  1.40s/episode, 总奖励=90.7, Epsilon=0.121] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  48%|████▊     | 480/1000 [08:25<10:27,  1.21s/episode, 总奖励=-5, Epsilon=0.09]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  51%|█████     | 506/1000 [09:08<15:45,  1.91s/episode, 总奖励=96.9, Epsilon=0.079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  58%|█████▊    | 581/1000 [11:14<12:25,  1.78s/episode, 总奖励=98.7, Epsilon=0.054]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  90%|█████████ | 902/1000 [20:24<02:30,  1.54s/episode, 总奖励=101, Epsilon=0.011]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 1000/1000 [23:49<00:00,  1.43s/episode, 总奖励=96.3, Epsilon=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "训练完成，模型已保存到 dqn_model.pth\n",
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Success Count: 748/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 0/1000 [00:00<?, ?episode/s, 总奖励=-10.3, Epsilon=0.995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 5/1000 [00:00<01:59,  8.30episode/s, 总奖励=-7.3, Epsilon=0.97]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   1%|          | 11/1000 [00:01<02:15,  7.29episode/s, 总奖励=-15.2, Epsilon=0.942]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  15%|█▍        | 147/1000 [00:46<06:49,  2.08episode/s, 总奖励=9.53, Epsilon=0.479] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  19%|█▉        | 191/1000 [01:15<07:14,  1.86episode/s, 总奖励=-3.55, Epsilon=0.384] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  24%|██▍       | 244/1000 [02:08<12:01,  1.05episode/s, 总奖励=-5, Epsilon=0.293]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  28%|██▊       | 284/1000 [02:32<07:52,  1.52episode/s, 总奖励=35.8, Epsilon=0.241] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  30%|███       | 303/1000 [02:49<13:42,  1.18s/episode, 总奖励=46.7, Epsilon=0.219]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  31%|███       | 307/1000 [02:54<15:24,  1.33s/episode, 总奖励=-5, Epsilon=0.214]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  34%|███▎      | 335/1000 [03:30<16:45,  1.51s/episode, 总奖励=55.6, Epsilon=0.187] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  34%|███▍      | 340/1000 [03:35<12:10,  1.11s/episode, 总奖励=0.8, Epsilon=0.181] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  39%|███▉      | 392/1000 [04:31<12:24,  1.22s/episode, 总奖励=69, Epsilon=0.14]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  42%|████▏     | 416/1000 [05:03<15:34,  1.60s/episode, 总奖励=80.3, Epsilon=0.124] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  45%|████▍     | 446/1000 [05:32<07:58,  1.16episode/s, 总奖励=83.8, Epsilon=0.107]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  49%|████▉     | 491/1000 [06:22<12:26,  1.47s/episode, 总奖励=83.9, Epsilon=0.085] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  50%|█████     | 505/1000 [06:34<09:52,  1.20s/episode, 总奖励=87.7, Epsilon=0.08]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  57%|█████▋    | 574/1000 [08:12<12:19,  1.74s/episode, 总奖励=88.4, Epsilon=0.056] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  58%|█████▊    | 576/1000 [08:14<10:30,  1.49s/episode, 总奖励=92, Epsilon=0.056]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  64%|██████▎   | 635/1000 [09:40<11:24,  1.88s/episode, 总奖励=92.4, Epsilon=0.041]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  68%|██████▊   | 684/1000 [10:52<08:18,  1.58s/episode, 总奖励=95.4, Epsilon=0.032] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  72%|███████▏  | 722/1000 [11:58<08:06,  1.75s/episode, 总奖励=96, Epsilon=0.027]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  73%|███████▎  | 732/1000 [12:16<08:18,  1.86s/episode, 总奖励=97, Epsilon=0.025]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 1000/1000 [19:32<00:00,  1.17s/episode, 总奖励=17.1, Epsilon=0.007] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "训练完成，模型已保存到 dqn_model.pth\n",
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Success Count: 635/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 2/1000 [00:00<01:27, 11.39episode/s, 总奖励=-7.3, Epsilon=0.985]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   2%|▏         | 22/1000 [00:04<03:44,  4.36episode/s, 总奖励=-5, Epsilon=0.896]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  14%|█▎        | 137/1000 [01:08<07:30,  1.92episode/s, 总奖励=3.6, Epsilon=0.501]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  19%|█▊        | 187/1000 [02:33<12:00,  1.13episode/s, 总奖励=21, Epsilon=0.392]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  21%|██        | 209/1000 [03:04<09:34,  1.38episode/s, 总奖励=-0.101, Epsilon=0.351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  24%|██▍       | 240/1000 [04:01<33:07,  2.61s/episode, 总奖励=28.1, Epsilon=0.3]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  30%|███       | 301/1000 [06:30<21:25,  1.84s/episode, 总奖励=-5, Epsilon=0.22]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  31%|███       | 310/1000 [07:04<58:40,  5.10s/episode, 总奖励=46.1, Epsilon=0.211] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  31%|███       | 311/1000 [07:11<1:02:05,  5.41s/episode, 总奖励=63.4, Epsilon=0.21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  40%|███▉      | 399/1000 [11:29<47:15,  4.72s/episode, 总奖励=65.7, Epsilon=0.135]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  41%|████      | 409/1000 [11:56<22:58,  2.33s/episode, 总奖励=73.2, Epsilon=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  46%|████▌     | 462/1000 [14:44<37:43,  4.21s/episode, 总奖励=82.1, Epsilon=0.099]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  55%|█████▍    | 546/1000 [19:06<31:45,  4.20s/episode, 总奖励=92.8, Epsilon=0.065] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  81%|████████  | 810/1000 [36:16<15:34,  4.92s/episode, 总奖励=93.1, Epsilon=0.017] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  95%|█████████▌| 950/1000 [46:08<03:56,  4.74s/episode, 总奖励=98.3, Epsilon=0.009] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度: 100%|██████████| 1000/1000 [49:17<00:00,  2.96s/episode, 总奖励=45.6, Epsilon=0.007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dqn_model.pth\n",
      "训练完成，模型已保存到 dqn_model.pth\n",
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Test Completed. Success Count: 578/1000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     env \u001b[38;5;241m=\u001b[39m GridWorld(grid_map , start\u001b[38;5;241m=\u001b[39mnodes[\u001b[38;5;241m0\u001b[39m] , goal\u001b[38;5;241m=\u001b[39mnodes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],goal_list\u001b[38;5;241m=\u001b[39mnodes,obs\u001b[38;5;241m=\u001b[39m\u001b[43mobst\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m], dobs\u001b[38;5;241m=\u001b[39mobst[i][\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      3\u001b[0m     agent \u001b[38;5;241m=\u001b[39m RainbowDQN(\n\u001b[0;32m      4\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m      5\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     memory_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     train_agent(env, agent, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    env = GridWorld(grid_map , start=nodes[0] , goal=nodes[-1],goal_list=nodes,obs=obst[i][0], dobs=obst[i][1])\n",
    "    agent = RainbowDQN(\n",
    "    env=env,\n",
    "    gamma=0.9,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.0001,\n",
    "    epsilon_decay=0.995,\n",
    "    learning_rate=0.005,\n",
    "    batch_size=64,\n",
    "    memory_size=200000\n",
    "    )\n",
    "    train_agent(env, agent, episodes=1000, max_steps=50000, save_path=\"dqn_model.pth\", log_dir=\"runs\")\n",
    "    env = GridWorld(grid_map , start=nodes[0] , goal=nodes[-1],goal_list=nodes,obs=obst[i][0], dobs=obst[i][1])\n",
    "    agent = RainbowDQN(\n",
    "        env=env,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        learning_rate=5e-3,\n",
    "        batch_size=64,\n",
    "        memory_size=20000\n",
    "    )\n",
    "\n",
    "    # 运行100次测试\n",
    "    check_trained_model_num(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        model_path=\"dqn_model.pth\",  # 替换为你的模型文件路径\n",
    "        max_steps=1000,\n",
    "        num_runs=1000,  # 运行1000次测试\n",
    "        result_file = f'./test/test_results{i}.txt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_70096\\3824810451.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from dqn_model.pth\n",
      "Loaded model from dqn_model.pth\n",
      "Obstacle added at (56, 97)\n",
      "Obstacle added at (55, 77)\n",
      "Obstacle added at (84, 81)\n",
      "Obstacle added at (80, 41)\n",
      "Obstacle added at (70, 15)\n",
      "Obstacle added at (48, 44)\n",
      "Obstacle added at (31, 22)\n",
      "Obstacle added at (16, 39)\n",
      "Obstacle added at (12, 68)\n",
      "Obstacle added at (79, 30)\n",
      "Obstacle added at (80, 30)\n",
      "Obstacle added at (51, 77)\n",
      "Obstacle added at (64, 80)\n",
      "Obstacle added at (83, 39)\n",
      "Obstacle added at (10, 46)\n",
      "Obstacle added at (14, 84)\n",
      "Test Completed. Total Reward: 196.6471298567925, Steps Taken: 288\n",
      "仿真完成，耗时 288.23 秒\n"
     ]
    }
   ],
   "source": [
    "# # 加载环境和模型\n",
    "# grid_map = np.loadtxt(files, dtype=int)\n",
    "# env = GridWorld(grid_map , start=nodes[0], goal=nodes[-1], goal_list=nodes)\n",
    "# agent = RainbowDQN(\n",
    "#     env=env,\n",
    "#     gamma=0.99,\n",
    "#     epsilon=1.0,\n",
    "#     epsilon_min=0.01,\n",
    "#     epsilon_decay=0.995,\n",
    "#     learning_rate=5e-3,\n",
    "#     batch_size=64,\n",
    "#     memory_size=20000\n",
    "# )\n",
    "# check_trained_model(\n",
    "#     env=env,\n",
    "#     agent=agent,\n",
    "#     model_path=\"dqn_model.pth\",  # 替换为你的模型文件路径\n",
    "#     max_steps=1000\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
