{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import copy\n",
    "import pygame\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, grid_map, start, goal, max_steps=50000, goal_list=None):\n",
    "        \"\"\"\n",
    "        初始化 GridWorld 环境\n",
    "        \"\"\"\n",
    "        self.grid_map = grid_map\n",
    "        self.base_grid_map = grid_map.copy()  # 保存基础地图（无动态障碍物）\n",
    "        self.rows, self.cols = grid_map.shape\n",
    "        self.max_steps = max_steps\n",
    "        self.goal_list = goal_list if goal_list else []  # 目标点列表\n",
    "        \n",
    "        if start is not None and goal is not None:\n",
    "            self.start = start\n",
    "            self.goal = goal\n",
    "            self.agent_pos = self.start\n",
    "            self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "        else:\n",
    "            self.reset_dynamic(start, goal)\n",
    "\n",
    "    def reset_dynamic(self, num_obstacles=30, num_dynamic_obstacles=10):\n",
    "        \"\"\"\n",
    "        每次 reset 时动态生成障碍物，并使用传入的起点和目标，或者随机生成。\n",
    "        \"\"\"\n",
    "        self.grid_map = self.base_grid_map.copy()\n",
    "\n",
    "        # 固定障碍物初始化\n",
    "        valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        random_obstacles = random.sample(valid_positions, num_obstacles)\n",
    "        for x, y in random_obstacles:\n",
    "            self.grid_map[x, y] = 1\n",
    "\n",
    "        # 动态障碍物初始化\n",
    "        self.dynamic_obstacles = []\n",
    "        for _ in range(num_dynamic_obstacles):\n",
    "            x, y = random.choice(valid_positions)\n",
    "            direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "            speed = random.randint(1, 2)\n",
    "\n",
    "            self.dynamic_obstacles.append({\n",
    "                'position': (x, y),\n",
    "                'direction': direction,\n",
    "                'speed': speed\n",
    "            })\n",
    "\n",
    "        # 确保起点和目标点不在障碍物中\n",
    "        self.valid_positions = [\n",
    "            (i, j) for i in range(1, self.rows - 1) for j in range(1, self.cols - 1)\n",
    "            if self.grid_map[i, j] == 0\n",
    "        ]\n",
    "        self.agent_pos = self.start\n",
    "        self.steps = 0\n",
    "        self.distance = np.linalg.norm(np.array(self.start) - np.array(self.goal))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置环境到初始状态\n",
    "        \"\"\"\n",
    "        self.reset_dynamic()\n",
    "        self.goal_list_copy = self.goal_list.copy()\n",
    "        self.current_goal_index = 0\n",
    "        self.goal = self.goal_list[self.current_goal_index] if self.goal_list else None\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self, n_frames=4):\n",
    "        \"\"\"\n",
    "        获取过去几帧的状态，生成时间序列。\n",
    "        \"\"\"\n",
    "        nearby_grid = np.ones((7, 7), dtype=int)\n",
    "        x_min, x_max = max(0, self.agent_pos[0] - 3), min(self.rows, self.agent_pos[0] + 4)\n",
    "        y_min, y_max = max(0, self.agent_pos[1] - 3), min(self.cols, self.agent_pos[1] + 4)\n",
    "        r_min, r_max = 3 - (self.agent_pos[0] - x_min), 3 + (x_max - self.agent_pos[0])\n",
    "        c_min, c_max = 3 - (self.agent_pos[1] - y_min), 3 + (y_max - self.agent_pos[1])\n",
    "\n",
    "        nearby_grid[r_min:r_max, c_min:c_max] = self.grid_map[x_min:x_max, y_min:y_max]\n",
    "        nearby_flat = nearby_grid.flatten()\n",
    "\n",
    "        dx = self.goal[0] - self.agent_pos[0]\n",
    "        dy = self.goal[1] - self.agent_pos[1]\n",
    "        distance_to_goal = np.sqrt(dx**2 + dy**2)\n",
    "        angle_to_goal = np.arctan2(dy, dx)\n",
    "\n",
    "        # 当前帧的状态\n",
    "        current_state = np.concatenate(([distance_to_goal, angle_to_goal], nearby_flat))\n",
    "        return current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作并更新环境状态\n",
    "        \"\"\"\n",
    "        state = self.get_state()\n",
    "        action = random.choices(range(9), weights=action, k=1)[0]\n",
    "        actions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1), (0 , 0)]\n",
    "        delta = actions[action]\n",
    "        next_pos = (self.agent_pos[0] + delta[0], self.agent_pos[1] + delta[1])\n",
    "\n",
    "        # 检查是否越界或碰到障碍物（包括动态障碍物和固定障碍物）\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward = -3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        if not (0 <= next_pos[0] < self.rows and 0 <= next_pos[1] < self.cols) or self.grid_map[next_pos] == 1:\n",
    "            reward = -3.0\n",
    "            done = True\n",
    "            return self.get_state(), reward, done\n",
    "\n",
    "        self.agent_pos = next_pos\n",
    "        reward = -0.5\n",
    "        done = False\n",
    "\n",
    "        next_distance = np.sqrt((self.goal[0] - self.agent_pos[0])**2 + (self.goal[1] - self.agent_pos[1])**2)\n",
    "        if(self.distance > next_distance):\n",
    "            reward += 0.6\n",
    "        else:\n",
    "            reward -= 0.4\n",
    "        self.distance = next_distance\n",
    "\n",
    "        action_vector = np.array([delta[0], delta[1]])\n",
    "        goal_vector = np.array([self.goal[0] - self.agent_pos[0], self.goal[1] - self.agent_pos[1]])\n",
    "        goal_vector_norm = goal_vector / (np.linalg.norm(goal_vector) + 1e-5)\n",
    "        \n",
    "        if np.linalg.norm(goal_vector) > 0.1:\n",
    "            alignment_reward = np.dot(action_vector, goal_vector_norm)\n",
    "        else:\n",
    "            alignment_reward = 0\n",
    "        \n",
    "        reward += alignment_reward * 0.4\n",
    "\n",
    "        if self.distance < 0.5:\n",
    "            reward += 10\n",
    "            if self.current_goal_index + 1 < len(self.goal_list):\n",
    "                self.current_goal_index += 1\n",
    "                self.goal = self.goal_list[self.current_goal_index]  \n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        # 更新动态障碍物的位置\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            x, y = obstacle['position']\n",
    "            direction = obstacle['direction']\n",
    "            speed = obstacle['speed']\n",
    "            \n",
    "            if direction == 'up':\n",
    "                new_pos = (x - speed, y)\n",
    "            elif direction == 'down':\n",
    "                new_pos = (x + speed, y)\n",
    "            elif direction == 'left':\n",
    "                new_pos = (x, y - speed)\n",
    "            elif direction == 'right':\n",
    "                new_pos = (x, y + speed)\n",
    "            \n",
    "            if 0 <= new_pos[0] < self.rows and 0 <= new_pos[1] < self.cols:\n",
    "                if self.grid_map[new_pos[0], new_pos[1]] == 1:\n",
    "                    if direction == 'up':\n",
    "                        obstacle['direction'] = 'down'\n",
    "                    elif direction == 'down':\n",
    "                        obstacle['direction'] = 'up'\n",
    "                    elif direction == 'left':\n",
    "                        obstacle['direction'] = 'right'\n",
    "                    elif direction == 'right':\n",
    "                        obstacle['direction'] = 'left'\n",
    "                else:\n",
    "                    # 更新障碍物的位置\n",
    "                    self.grid_map[x, y] = 0  # 清除旧位置\n",
    "                    self.grid_map[new_pos[0], new_pos[1]] = 1  # 设置新位置\n",
    "                    obstacle['position'] = new_pos\n",
    "\n",
    "        # 计算最小障碍物距离\n",
    "        min_distance_to_obstacle = float('inf')\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.cols):\n",
    "                if self.grid_map[i, j] == 1:  # 障碍物\n",
    "                    distance_to_obstacle = np.linalg.norm(np.array(self.agent_pos) - np.array((i, j)))\n",
    "                    min_distance_to_obstacle = min(min_distance_to_obstacle, distance_to_obstacle)\n",
    "\n",
    "        # 奖励：距离障碍物越近，惩罚越大\n",
    "        if min_distance_to_obstacle == 1.0:\n",
    "            reward -= 2.0\n",
    "        elif min_distance_to_obstacle <= 2.0:\n",
    "            reward -= 1.5\n",
    "        elif min_distance_to_obstacle <= 3.0:\n",
    "            reward -= 0.2\n",
    "        else :\n",
    "            reward -= 0.01\n",
    "\n",
    "        # 检查是否碰到动态障碍物\n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if self.agent_pos == obstacle['position']:\n",
    "                reward -= 3.0\n",
    "                done = True\n",
    "                return self.get_state(), reward, done\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "            reward -= 10\n",
    "\n",
    "        return self.get_state(), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return F.softmax(self.l3(a), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=20000):\n",
    "        \"\"\"\n",
    "        初始化经验回放缓冲区\n",
    "        :param max_size: 缓冲区的最大容量\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.storage = []\n",
    "        self.ptr = 0\n",
    "\n",
    "    def append(self, transition):\n",
    "        \"\"\"\n",
    "        添加新的经验到缓冲区\n",
    "        :param transition: (state, action, reward, next_state, done)\n",
    "        \"\"\"\n",
    "        # 检查是否存在 NaN 或非法值\n",
    "        state, action, reward, next_state, done = transition\n",
    "        if np.any(np.isnan(state)) or np.any(np.isnan(action)) or np.isnan(reward) or np.any(np.isnan(next_state)):\n",
    "            print(\"Invalid data detected in transition. Skipping storage:\")\n",
    "            print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}, Done: {done}\")\n",
    "            return\n",
    "\n",
    "        # 如果缓冲区未满，则追加新位置\n",
    "        if len(self.storage) < self.max_size:\n",
    "            self.storage.append(None)\n",
    "        \n",
    "        # 存储数据并更新指针\n",
    "        self.storage[self.ptr] = (state, action, reward, next_state, done)\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        随机采样一批经验\n",
    "        :param batch_size: 批量大小\n",
    "        :return: 批量经验 (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for i in indices:\n",
    "            state, action, reward, next_state, done = self.storage[i]\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)  # 保持标量\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)  # 保持标量\n",
    "\n",
    "        # 返回可解包的元组\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回缓冲区中存储的经验数量。\n",
    "        \"\"\"\n",
    "        return len(self.storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3:\n",
    "    def __init__(self, state_dim, action_dim, actor_lr=2e-4, critic_lr=2e-3, gamma=0.8, tau=0.005, device=\"cuda\",policy_noise=0.2, epsilon=1.0, epsilon_min=0.001, epsilon_decay=0.95,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2):\n",
    "        # 初始化 Actor 和 Critic\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # 优化器\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # 超参数\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # 经验回放缓冲区\n",
    "        self.replay_buffer = ReplayBuffer(max_size=100000)\n",
    "\n",
    "        # 设备信息\n",
    "        self.device = device\n",
    "        self.total_it = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        probs = self.actor(state).cpu().data.numpy().flatten()\n",
    "        return probs\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, float(done)))\n",
    "        \n",
    "    def soft_update(self, source, target):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(self.tau * source_param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def train(self, batch_size=256):\n",
    "        if len(self.replay_buffer.storage) < batch_size:\n",
    "            return None, None  # 如果缓冲区样本不足，返回 None\n",
    "\n",
    "        # 从 ReplayBuffer 中采样\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # 转换为 Tensor\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # Critic 更新\n",
    "        with torch.no_grad():\n",
    "            #noise = (\n",
    "\t\t\t#\ttorch.randn_like(actions) * self.policy_noise\n",
    "\t\t\t#).clamp(-self.noise_clip, self.noise_clip)\n",
    "            #next_actions = (\n",
    "\t\t\t#\tself.actor_target(next_states) + noise\n",
    "\t\t\t#)\n",
    "            next_actions = self.actor_target(next_states)\n",
    "            target_Q1, target_Q2 = self.critic_target(next_states, next_actions)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = rewards + (1 - dones) * self.gamma * target_Q\n",
    "            \n",
    "        current_Q1, current_Q2 = self.critic(states, actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor 更新\n",
    "        actor_loss = -self.critic.Q1(states, self.actor(states)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新目标网络\n",
    "        self.soft_update(self.actor, self.actor_target)\n",
    "        self.soft_update(self.critic, self.critic_target)\n",
    "    \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, actor_path=\"actor.pth\", critic_path=\"critic.pth\"):\n",
    "        torch.save(self.actor.state_dict(), actor_path)\n",
    "        torch.save(self.critic.state_dict(), critic_path)\n",
    "        print(f\"Actor model saved to {actor_path}\")\n",
    "        print(f\"Critic model saved to {critic_path}\")\n",
    "\n",
    "    def load_model(self, actor_path=\"actor.pth\", critic_path=\"critic.pth\"):\n",
    "        self.actor.load_state_dict(torch.load(actor_path))\n",
    "        self.critic.load_state_dict(torch.load(critic_path))\n",
    "        print(f\"Actor model loaded from {actor_path}\")\n",
    "        print(f\"Critic model loaded from {critic_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=1000, max_steps=1000, actor_path=\"actor.pth\", critic_path=\"critic.pth\", log_dir=\"runs\"):\n",
    "    \"\"\"\n",
    "    使用 DDPG 训练智能体并保存模型。\n",
    "    \"\"\"\n",
    "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = os.path.join(log_dir, f\"run_{run_id}\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    with tqdm(total=episodes, desc=\"训练进度\", unit=\"episode\") as pbar:\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            episode_steps = 0\n",
    "\n",
    "            while not done:\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.store_transition(state, action, reward, next_state, done)\n",
    "                actor_loss, critic_loss = agent.train(batch_size=256)  # 获取损失值\n",
    "                state = next_state\n",
    "\n",
    "                total_reward += reward\n",
    "                episode_steps += 1\n",
    "\n",
    "                if episode_steps >= max_steps:\n",
    "                    done = True\n",
    "                    \n",
    "            # 每个回合结束后，衰减 epsilon（探索率）\n",
    "            agent.decay_epsilon()    \n",
    "            # 记录到 TensorBoard\n",
    "            writer.add_scalar(\"Reward/Episode\", total_reward, episode)\n",
    "            if actor_loss is not None and critic_loss is not None:\n",
    "                writer.add_scalar(\"Loss/Actor\", actor_loss, episode)\n",
    "                writer.add_scalar(\"Loss/Critic\", critic_loss, episode)\n",
    "\n",
    "            pbar.set_postfix({\"总奖励\": total_reward})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # 保存模型\n",
    "            if total_reward > best_reward:\n",
    "                best_reward = total_reward\n",
    "                agent.save_model(actor_path, critic_path)\n",
    "\n",
    "        # 保存最终模型\n",
    "        agent.save_model(actor_path, critic_path)\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"训练完成，Actor 模型保存至 {actor_path}，Critic 模型保存至 {critic_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 1/10000 [00:00<44:20,  3.76episode/s, 总奖励=-6.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor model saved to ddpg_actor.pth\n",
      "Critic model saved to ddpg_critic.pth\n",
      "Actor model saved to ddpg_actor.pth\n",
      "Critic model saved to ddpg_critic.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 7/10000 [00:01<18:24,  9.05episode/s, 总奖励=-16.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor model saved to ddpg_actor.pth\n",
      "Critic model saved to ddpg_critic.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:   0%|          | 12/10000 [00:01<11:18, 14.73episode/s, 总奖励=-12.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor model saved to ddpg_actor.pth\n",
      "Critic model saved to ddpg_critic.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练进度:  12%|█▏        | 1151/10000 [00:13<01:42, 86.47episode/s, 总奖励=-3] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m TD3(state_dim, action_dim)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mddpg_actor.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 保存 Actor 的路径\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcritic_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mddpg_critic.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 保存 Critic 的路径\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(env, agent, episodes, max_steps, actor_path, critic_path, log_dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     22\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n\u001b[1;32m---> 23\u001b[0m actor_loss, critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 获取损失值\u001b[39;00m\n\u001b[0;32m     24\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     26\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[6], line 79\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     76\u001b[0m critic_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(current_Q1, target_Q) \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(current_Q2, target_Q)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 79\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Actor 更新\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\lab\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\lab\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\lab\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_map = np.loadtxt(\"grid_map_final.txt\", dtype=int)\n",
    "nodes=[(51, 12), (55, 23), (55, 30), (32, 47), (9, 50), (9, 64)]\n",
    "env = GridWorld(grid_map , start=(58, 5) , goal=(51, 12),goal_list=nodes)\n",
    "\n",
    "# 实例化 DDPG Agent\n",
    "state_dim = 51\n",
    "action_dim = 9 \n",
    "agent = TD3(state_dim, action_dim)\n",
    "# 开始训练\n",
    "train_agent(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    episodes=10000,\n",
    "    max_steps=50000,\n",
    "    actor_path=\"ddpg_actor.pth\",  # 保存 Actor 的路径\n",
    "    critic_path=\"ddpg_critic.pth\",  # 保存 Critic 的路径\n",
    "    log_dir=\"runs\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
